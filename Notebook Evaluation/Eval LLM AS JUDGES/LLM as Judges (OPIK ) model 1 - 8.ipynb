{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d104b9ddd845488f9adf66ee2fc4d392": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0406b028c2248549526c5472d3e79c6",
              "IPY_MODEL_685a89fb03e648e18fc0ead76fc5dbd5",
              "IPY_MODEL_e77d727c58f54a99bff29e9ec499cf8c"
            ],
            "layout": "IPY_MODEL_e2cb4a0a5baf46ddaf9c8433c53db864"
          }
        },
        "d0406b028c2248549526c5472d3e79c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea093126ed644386a6212df8a8ed087e",
            "placeholder": "​",
            "style": "IPY_MODEL_02e9405a6e674a97b7c2ebf54c88a41f",
            "value": "Evaluation:  99%"
          }
        },
        "685a89fb03e648e18fc0ead76fc5dbd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6b9bc7f99e14187b8f85424eb48be3d",
            "max": 139,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b45779bb97d478ba495c34e02277a64",
            "value": 138
          }
        },
        "e77d727c58f54a99bff29e9ec499cf8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bea5aabb0cc142d595e554554394129d",
            "placeholder": "​",
            "style": "IPY_MODEL_d2c8307ac52f4025abfa4f90865c5f6f",
            "value": " 138/139 [10:25&lt;00:00,  2.28it/s]"
          }
        },
        "e2cb4a0a5baf46ddaf9c8433c53db864": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea093126ed644386a6212df8a8ed087e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02e9405a6e674a97b7c2ebf54c88a41f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6b9bc7f99e14187b8f85424eb48be3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b45779bb97d478ba495c34e02277a64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bea5aabb0cc142d595e554554394129d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2c8307ac52f4025abfa4f90865c5f6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a1a11805c6c46429156045e77d9a6e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7499ad6d96ef477aaaa5c37e3d13db20",
              "IPY_MODEL_7ba5d1a92969426991d45530202ad424",
              "IPY_MODEL_c29f0cced4734558bd9c234fb4de2043"
            ],
            "layout": "IPY_MODEL_0187071be4f94727925d2d07a360fc9d"
          }
        },
        "7499ad6d96ef477aaaa5c37e3d13db20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c10e4e60c5a434ba2decad2e5a30f09",
            "placeholder": "​",
            "style": "IPY_MODEL_836f20e3643545baa3af82e0a6ccb5a9",
            "value": "Evaluation: 100%"
          }
        },
        "7ba5d1a92969426991d45530202ad424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71a39dcae0cb48e3a095a39c335555ae",
            "max": 139,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bdfa53cb79fd49b0b539af65f074651f",
            "value": 139
          }
        },
        "c29f0cced4734558bd9c234fb4de2043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34fa9642fa6648b3bd2de5d9dd00f154",
            "placeholder": "​",
            "style": "IPY_MODEL_8308a9461dcc42e4a6d24e96a27e7a21",
            "value": " 139/139 [04:48&lt;00:00,  2.30s/it]"
          }
        },
        "0187071be4f94727925d2d07a360fc9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c10e4e60c5a434ba2decad2e5a30f09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "836f20e3643545baa3af82e0a6ccb5a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71a39dcae0cb48e3a095a39c335555ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdfa53cb79fd49b0b539af65f074651f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "34fa9642fa6648b3bd2de5d9dd00f154": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8308a9461dcc42e4a6d24e96a27e7a21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3dbfeea9f13d46929862420652999b17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06a90c6c8b5e4153abf4b6c4e4f24e1c",
              "IPY_MODEL_e956672642e54fefa31be8b4c6ecf839",
              "IPY_MODEL_ffc1d54e95e44aea9a323a2ed29f8976"
            ],
            "layout": "IPY_MODEL_a0adc22938a04c8fa26e8c0330aaf1d0"
          }
        },
        "06a90c6c8b5e4153abf4b6c4e4f24e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_514cc126d9b143f89123a60c16ebb4d2",
            "placeholder": "​",
            "style": "IPY_MODEL_b148b6b2425c4251923057676135f803",
            "value": "Evaluation: 100%"
          }
        },
        "e956672642e54fefa31be8b4c6ecf839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cecdd04f9b7a4d96acfd8a5a1026c233",
            "max": 148,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bf6b793a162483fa0c7f38aae129685",
            "value": 148
          }
        },
        "ffc1d54e95e44aea9a323a2ed29f8976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9dfd14099da42f0b72484050525079f",
            "placeholder": "​",
            "style": "IPY_MODEL_f7c7bc2666ee4b61ad34bef0cca56fd3",
            "value": " 148/148 [05:58&lt;00:00,  2.08s/it]"
          }
        },
        "a0adc22938a04c8fa26e8c0330aaf1d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "514cc126d9b143f89123a60c16ebb4d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b148b6b2425c4251923057676135f803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cecdd04f9b7a4d96acfd8a5a1026c233": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bf6b793a162483fa0c7f38aae129685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9dfd14099da42f0b72484050525079f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7c7bc2666ee4b61ad34bef0cca56fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08de8bdfacc34c0bb0f106e220ecb296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb7d39c2963f498caeff6ecd1199a619",
              "IPY_MODEL_de4d019568ce4917ab6b0418db02cbef",
              "IPY_MODEL_4bfed8c41b834c259b9b9be3427d8e89"
            ],
            "layout": "IPY_MODEL_95e509460e2a4ce885a512a53266996a"
          }
        },
        "eb7d39c2963f498caeff6ecd1199a619": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9acfb0fd62ee42fbb27ceb356ab4b5d2",
            "placeholder": "​",
            "style": "IPY_MODEL_d049a99430e843de85db9d46bf9ea149",
            "value": "Evaluation: "
          }
        },
        "de4d019568ce4917ab6b0418db02cbef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_545788f9c285406a9e472bd5fe6b999d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd3d034908794415ae1b54311354ba86",
            "value": 0
          }
        },
        "4bfed8c41b834c259b9b9be3427d8e89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5935bd77e8a48288325db3d7626b450",
            "placeholder": "​",
            "style": "IPY_MODEL_fa1c2e27689b4b2e95d13595012b9c4b",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "95e509460e2a4ce885a512a53266996a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9acfb0fd62ee42fbb27ceb356ab4b5d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d049a99430e843de85db9d46bf9ea149": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "545788f9c285406a9e472bd5fe6b999d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "cd3d034908794415ae1b54311354ba86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5935bd77e8a48288325db3d7626b450": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa1c2e27689b4b2e95d13595012b9c4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67f6ed09ab9a4d1cba57bc558bd9c3e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4e11f6c3f45464ebb11a85b9f3f6aec",
              "IPY_MODEL_444d1d2228094774b53618e833673441",
              "IPY_MODEL_b20275da1c0f4902953f1a5e7b358489"
            ],
            "layout": "IPY_MODEL_d058d8e588b449cc90df1de29e3396ca"
          }
        },
        "b4e11f6c3f45464ebb11a85b9f3f6aec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_445a99cf5d0046b4a5bb1388cdbaf7a9",
            "placeholder": "​",
            "style": "IPY_MODEL_36411d5c8f454b93aca7c1f2ac5d6efb",
            "value": "Evaluation: "
          }
        },
        "444d1d2228094774b53618e833673441": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c76152a36c564709a76e1b60385bb152",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_453bafbb626e48d48f1f91bd25ca8a53",
            "value": 0
          }
        },
        "b20275da1c0f4902953f1a5e7b358489": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_376a49566e2e4d9d95ee274b12ec7a33",
            "placeholder": "​",
            "style": "IPY_MODEL_7588ec39b4334aa1abefbb5dbdcadeb5",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "d058d8e588b449cc90df1de29e3396ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "445a99cf5d0046b4a5bb1388cdbaf7a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36411d5c8f454b93aca7c1f2ac5d6efb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c76152a36c564709a76e1b60385bb152": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "453bafbb626e48d48f1f91bd25ca8a53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "376a49566e2e4d9d95ee274b12ec7a33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7588ec39b4334aa1abefbb5dbdcadeb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6faf2c1a382f4201a819ee6342a1fd3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d79932ad83f47caaa60520457a549b6",
              "IPY_MODEL_1313bf7505f34e1e848f5aa44f75732c",
              "IPY_MODEL_17dfeb5971ed44aa8ce04267d1d30248"
            ],
            "layout": "IPY_MODEL_4fe5db0d59994d7aabe8c5a6cd1194ea"
          }
        },
        "6d79932ad83f47caaa60520457a549b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0b8ade8311c4a5ea59ae6ccee06c2e1",
            "placeholder": "​",
            "style": "IPY_MODEL_aa166e5985de4fbda3dedd8dee9a6a9d",
            "value": "Evaluation: 100%"
          }
        },
        "1313bf7505f34e1e848f5aa44f75732c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbaf787b057f4e1091781e6dd05d3a1d",
            "max": 101,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3bce94f3764b43bb87e49ecacf7a0dcb",
            "value": 101
          }
        },
        "17dfeb5971ed44aa8ce04267d1d30248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81857923a73f472499def6f33b4a5b70",
            "placeholder": "​",
            "style": "IPY_MODEL_04a27c01cdd341b2888ea91ee89d766f",
            "value": " 101/101 [04:37&lt;00:00,  2.74s/it]"
          }
        },
        "4fe5db0d59994d7aabe8c5a6cd1194ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0b8ade8311c4a5ea59ae6ccee06c2e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa166e5985de4fbda3dedd8dee9a6a9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbaf787b057f4e1091781e6dd05d3a1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bce94f3764b43bb87e49ecacf7a0dcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81857923a73f472499def6f33b4a5b70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04a27c01cdd341b2888ea91ee89d766f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90e9acba5bec4701872437fc7e425caa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9914b0133d954a24a85428a26f6df09c",
              "IPY_MODEL_4e957f5441584b599c6e6d3f17dbcfbf",
              "IPY_MODEL_f47d2a5d6ba7417f80f4b253c2e9819c"
            ],
            "layout": "IPY_MODEL_e6351dd6bde24cf7975e04389469f5b3"
          }
        },
        "9914b0133d954a24a85428a26f6df09c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64fb8783645b497ea6f56a9848e826b2",
            "placeholder": "​",
            "style": "IPY_MODEL_245e2beca3204261a0ff6070b5ff0936",
            "value": "Evaluation: 100%"
          }
        },
        "4e957f5441584b599c6e6d3f17dbcfbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3515364d471d4473a50a87c94103a442",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a9efc4888cd4b5da7ae5d0be6df7171",
            "value": 100
          }
        },
        "f47d2a5d6ba7417f80f4b253c2e9819c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66053092ffb043489ccb37c7ddfb8913",
            "placeholder": "​",
            "style": "IPY_MODEL_caae8be46e7f4f639a11ec83eead7732",
            "value": " 100/100 [04:12&lt;00:00,  1.93s/it]"
          }
        },
        "e6351dd6bde24cf7975e04389469f5b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64fb8783645b497ea6f56a9848e826b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "245e2beca3204261a0ff6070b5ff0936": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3515364d471d4473a50a87c94103a442": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a9efc4888cd4b5da7ae5d0be6df7171": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66053092ffb043489ccb37c7ddfb8913": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "caae8be46e7f4f639a11ec83eead7732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa9064e7470043e7a600250541c02845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d7426f2d8df4aa1b24e96efeb28e40c",
              "IPY_MODEL_ddd7cbf268794600865ae5e06d606e09",
              "IPY_MODEL_5cba9496b49740a5b789123aac39e9ca"
            ],
            "layout": "IPY_MODEL_34b2f919efe64b8fb5e7b03fe04abfff"
          }
        },
        "7d7426f2d8df4aa1b24e96efeb28e40c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7d8016b062941bf9353bda0cb14b88c",
            "placeholder": "​",
            "style": "IPY_MODEL_0d4d1ab282034baa93581fecfdb9f861",
            "value": "Evaluation: 100%"
          }
        },
        "ddd7cbf268794600865ae5e06d606e09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ef6549244ee4c8fb5940c749ef5d525",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae44c0b70d1542b5b6fa83a62da7358d",
            "value": 100
          }
        },
        "5cba9496b49740a5b789123aac39e9ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc2975b0a5904fefa3ce2552df1e09b0",
            "placeholder": "​",
            "style": "IPY_MODEL_d19070d5716c4fa383414bfe65e0318b",
            "value": " 100/100 [03:56&lt;00:00,  2.70s/it]"
          }
        },
        "34b2f919efe64b8fb5e7b03fe04abfff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7d8016b062941bf9353bda0cb14b88c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d4d1ab282034baa93581fecfdb9f861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ef6549244ee4c8fb5940c749ef5d525": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae44c0b70d1542b5b6fa83a62da7358d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc2975b0a5904fefa3ce2552df1e09b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d19070d5716c4fa383414bfe65e0318b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba2e73cf373d4925be71ace81ab9004e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6146d7e2aa9c4ccbab229478016d9fd4",
              "IPY_MODEL_d734b8fab0194956bf567a34bbfd1839",
              "IPY_MODEL_eb3e36873440484590d24bd49b13fdda"
            ],
            "layout": "IPY_MODEL_bdd9c6d6e761480c85a265d0d3a14a48"
          }
        },
        "6146d7e2aa9c4ccbab229478016d9fd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00ebfe6b519e468298a97cf2e15ea85a",
            "placeholder": "​",
            "style": "IPY_MODEL_2546662abbc940c8b3e94b7226ba3d45",
            "value": "Evaluation: 100%"
          }
        },
        "d734b8fab0194956bf567a34bbfd1839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e720acb49b214318b19c5da48dc9a171",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17f7c2f75d794ded9c2d03b938164829",
            "value": 100
          }
        },
        "eb3e36873440484590d24bd49b13fdda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a0b3ce4b63c49fba93044234a58a9e0",
            "placeholder": "​",
            "style": "IPY_MODEL_fe80ae0d76144be3a8463c8e6325bb9f",
            "value": " 100/100 [03:49&lt;00:00,  2.66s/it]"
          }
        },
        "bdd9c6d6e761480c85a265d0d3a14a48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00ebfe6b519e468298a97cf2e15ea85a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2546662abbc940c8b3e94b7226ba3d45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e720acb49b214318b19c5da48dc9a171": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17f7c2f75d794ded9c2d03b938164829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a0b3ce4b63c49fba93044234a58a9e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe80ae0d76144be3a8463c8e6325bb9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9123bdf6fd2a4f6499ebaad623599525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31284da60c4d489c8c2553b2906e314b",
              "IPY_MODEL_332d3491e0db4d44a23af7804da9ab27",
              "IPY_MODEL_401e8f803a024a4e9964cf6326a00801"
            ],
            "layout": "IPY_MODEL_8b13e8c56078400b821f4f87ae9dea15"
          }
        },
        "31284da60c4d489c8c2553b2906e314b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2efd647e822427aac87784e4cf25951",
            "placeholder": "​",
            "style": "IPY_MODEL_0275cb7f4ee1439d93abde6247bd129a",
            "value": "Evaluation: 100%"
          }
        },
        "332d3491e0db4d44a23af7804da9ab27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8908f862324402ca0f13220c8939ea1",
            "max": 105,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_341df57755ab4858916d09d04c38b479",
            "value": 105
          }
        },
        "401e8f803a024a4e9964cf6326a00801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0da5d2f1e5924519ba4f47f9240e1948",
            "placeholder": "​",
            "style": "IPY_MODEL_32540666c2c04146bfbc8d5da9e09b1c",
            "value": " 105/105 [05:47&lt;00:00,  4.94s/it]"
          }
        },
        "8b13e8c56078400b821f4f87ae9dea15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2efd647e822427aac87784e4cf25951": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0275cb7f4ee1439d93abde6247bd129a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8908f862324402ca0f13220c8939ea1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "341df57755ab4858916d09d04c38b479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0da5d2f1e5924519ba4f47f9240e1948": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32540666c2c04146bfbc8d5da9e09b1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4a8b19f3d964dcdb685108c5190dab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7977c0d18923497191838e85322ae5c4",
              "IPY_MODEL_026cd0dc33854162b855c337a07368a4",
              "IPY_MODEL_0a0a219b86bb4287b192971a9ec71eaf"
            ],
            "layout": "IPY_MODEL_c205458fb65c4703851498e10f867c1c"
          }
        },
        "7977c0d18923497191838e85322ae5c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5271bd46876432c9bd9f3beb3f173e0",
            "placeholder": "​",
            "style": "IPY_MODEL_41c4899af13d4fa4a395af0c4e35b1ad",
            "value": "Evaluation: 100%"
          }
        },
        "026cd0dc33854162b855c337a07368a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52fe4009251446c098fccc37186ef916",
            "max": 103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f0c788b62f44817bdfceaeb0a4c133d",
            "value": 103
          }
        },
        "0a0a219b86bb4287b192971a9ec71eaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f731349008e404fa40105e7407e6e30",
            "placeholder": "​",
            "style": "IPY_MODEL_32cdb491f87b40e59d1175abedd347a2",
            "value": " 103/103 [05:23&lt;00:00,  3.18s/it]"
          }
        },
        "c205458fb65c4703851498e10f867c1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5271bd46876432c9bd9f3beb3f173e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41c4899af13d4fa4a395af0c4e35b1ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52fe4009251446c098fccc37186ef916": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f0c788b62f44817bdfceaeb0a4c133d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f731349008e404fa40105e7407e6e30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32cdb491f87b40e59d1175abedd347a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb66eb60b2534694af93da50df0c96d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_448947bc6b734e618535f95bca15936c",
              "IPY_MODEL_a502846b271a47ec903c186c9a488be7",
              "IPY_MODEL_428874711bf14c5997e51b0cb3448473"
            ],
            "layout": "IPY_MODEL_5736d85e260a4b2f8528cff3b0504015"
          }
        },
        "448947bc6b734e618535f95bca15936c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03eabaa93f5d4763b785ab4f6af42d0e",
            "placeholder": "​",
            "style": "IPY_MODEL_6be7a79e460c4f85bfd8e37f0c616292",
            "value": "Evaluation: 100%"
          }
        },
        "a502846b271a47ec903c186c9a488be7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e7d4c366b56438c94d71377ac1ee98a",
            "max": 102,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64929cff9c4b49199bf5c50b1b4617fd",
            "value": 102
          }
        },
        "428874711bf14c5997e51b0cb3448473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecd823e8dddd4205aa17b5967ddec1fe",
            "placeholder": "​",
            "style": "IPY_MODEL_20316f0a4a37469a8e3d45c9a62f444e",
            "value": " 102/102 [05:32&lt;00:00,  3.28s/it]"
          }
        },
        "5736d85e260a4b2f8528cff3b0504015": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03eabaa93f5d4763b785ab4f6af42d0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6be7a79e460c4f85bfd8e37f0c616292": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e7d4c366b56438c94d71377ac1ee98a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64929cff9c4b49199bf5c50b1b4617fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ecd823e8dddd4205aa17b5967ddec1fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20316f0a4a37469a8e3d45c9a62f444e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2336def77d5464d9397f0d9f8e50c36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_264359af786e4617bfc1a9c5c60018bf",
              "IPY_MODEL_5c29b73554f743e2b8a31d2dc47867c7",
              "IPY_MODEL_d27c9bd1674a462e9bb7e967699e2371"
            ],
            "layout": "IPY_MODEL_4d517158e1794885bbbc5fd839376edf"
          }
        },
        "264359af786e4617bfc1a9c5c60018bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fa4ef428bf940aebfdc5a9d211a379b",
            "placeholder": "​",
            "style": "IPY_MODEL_5a5a0aabd6b24b78acfd01fd3972f189",
            "value": "Evaluation: 100%"
          }
        },
        "5c29b73554f743e2b8a31d2dc47867c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_012cd3f03cd9472cae61fa510b2cac20",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3eb3b13e7340478eaf5969aeab0d1714",
            "value": 100
          }
        },
        "d27c9bd1674a462e9bb7e967699e2371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dda195906bae425d982d0cb0a236e421",
            "placeholder": "​",
            "style": "IPY_MODEL_39a90ec2ef1d403b9633b908ab01b6df",
            "value": " 100/100 [05:25&lt;00:00,  4.39s/it]"
          }
        },
        "4d517158e1794885bbbc5fd839376edf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fa4ef428bf940aebfdc5a9d211a379b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a5a0aabd6b24b78acfd01fd3972f189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "012cd3f03cd9472cae61fa510b2cac20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3eb3b13e7340478eaf5969aeab0d1714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dda195906bae425d982d0cb0a236e421": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39a90ec2ef1d403b9633b908ab01b6df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec64b03858c348f685a790ff9cee8f5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7977e7ca7ab4902a9f47b5a9069c643",
              "IPY_MODEL_6886b2dfecee4cc5a0d44404f0410870",
              "IPY_MODEL_da85f85144b44d6eb84570d751ba4ee8"
            ],
            "layout": "IPY_MODEL_2ea3542481ec4fbeb91fc12ba1ed46ea"
          }
        },
        "c7977e7ca7ab4902a9f47b5a9069c643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_252323ecc4b540a2b375c47caf03f516",
            "placeholder": "​",
            "style": "IPY_MODEL_f8ac0876102b4666a6207d3ddd89115a",
            "value": "Evaluation: 100%"
          }
        },
        "6886b2dfecee4cc5a0d44404f0410870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_926cf1d062184ecba32f2a571ac78aaa",
            "max": 104,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d0044d04c364de58e076c984e86fd3b",
            "value": 104
          }
        },
        "da85f85144b44d6eb84570d751ba4ee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3491f8eaf49d4a15bbdf1e38bfe2640f",
            "placeholder": "​",
            "style": "IPY_MODEL_1aa2cb4b0aa243ca8ebebffa23923f62",
            "value": " 104/104 [06:38&lt;00:00,  6.00s/it]"
          }
        },
        "2ea3542481ec4fbeb91fc12ba1ed46ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "252323ecc4b540a2b375c47caf03f516": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8ac0876102b4666a6207d3ddd89115a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "926cf1d062184ecba32f2a571ac78aaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d0044d04c364de58e076c984e86fd3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3491f8eaf49d4a15bbdf1e38bfe2640f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1aa2cb4b0aa243ca8ebebffa23923f62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85c6e8eca0d24ed0ad3beec204777886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5311a9ddf604384b02d058c71a6bf50",
              "IPY_MODEL_2bbb427cca6e494c911684541813804d",
              "IPY_MODEL_86f8d3c8f2fc4c6b89bd0d212327bd29"
            ],
            "layout": "IPY_MODEL_a110e737742241c0babc249f6cd5f072"
          }
        },
        "b5311a9ddf604384b02d058c71a6bf50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b306550c369a4d34bb0bb58d2c819d2f",
            "placeholder": "​",
            "style": "IPY_MODEL_5c5c3ade08854ac8b55558dd2e074a20",
            "value": "Evaluation: 100%"
          }
        },
        "2bbb427cca6e494c911684541813804d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e87f65fd79a64cd5988ea9840c5901e6",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2077281d80a4223924f50f2091be55f",
            "value": 100
          }
        },
        "86f8d3c8f2fc4c6b89bd0d212327bd29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d5a81b20aae444380a2dc09d4ab2249",
            "placeholder": "​",
            "style": "IPY_MODEL_c51c55b8b3aa46fcbe4db85691275a6f",
            "value": " 100/100 [06:33&lt;00:00,  2.85s/it]"
          }
        },
        "a110e737742241c0babc249f6cd5f072": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b306550c369a4d34bb0bb58d2c819d2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c5c3ade08854ac8b55558dd2e074a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e87f65fd79a64cd5988ea9840c5901e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2077281d80a4223924f50f2091be55f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d5a81b20aae444380a2dc09d4ab2249": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c51c55b8b3aa46fcbe4db85691275a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e300af583fa4a8a8710003c90ee4c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eebd1245c55f44c28346ecd2563454b3",
              "IPY_MODEL_d8ef5c819e9a49e49beaa4784ac09f66",
              "IPY_MODEL_d0c0b2730c3d4f5980c295465d54dfc1"
            ],
            "layout": "IPY_MODEL_fae8f47216644fe29b6f410a54c926cc"
          }
        },
        "eebd1245c55f44c28346ecd2563454b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_192c0c19572f4467ae6ed90e2a538616",
            "placeholder": "​",
            "style": "IPY_MODEL_4ed4dd97c5c34f88b8f79550949c9de1",
            "value": "Evaluation: 100%"
          }
        },
        "d8ef5c819e9a49e49beaa4784ac09f66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b1d342532a44ad0bc544bda922c70e3",
            "max": 104,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf8543ad2fdd457e98592efee8f9c618",
            "value": 104
          }
        },
        "d0c0b2730c3d4f5980c295465d54dfc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a72825493fdf41e19e9977cb5b9fcc08",
            "placeholder": "​",
            "style": "IPY_MODEL_c1dad15e1f4d402799c47b0d2812afc2",
            "value": " 104/104 [06:49&lt;00:00,  4.67s/it]"
          }
        },
        "fae8f47216644fe29b6f410a54c926cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "192c0c19572f4467ae6ed90e2a538616": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ed4dd97c5c34f88b8f79550949c9de1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b1d342532a44ad0bc544bda922c70e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf8543ad2fdd457e98592efee8f9c618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a72825493fdf41e19e9977cb5b9fcc08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1dad15e1f4d402799c47b0d2812afc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e190a45d69ac43e6b8c5de5a5572d858": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77be98c82c6845a9a810567103ab0c10",
              "IPY_MODEL_3ec72483447a4662863ce1fa37ab243d",
              "IPY_MODEL_3baa14253d684e249a1c50dafd13c7a0"
            ],
            "layout": "IPY_MODEL_5215a6d2f29240e6a97f945a0d0bc7a1"
          }
        },
        "77be98c82c6845a9a810567103ab0c10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6b6152eda8f41349b7842fc012a7c23",
            "placeholder": "​",
            "style": "IPY_MODEL_d5ebd97b353d43bd8808249a3358b2d0",
            "value": "Evaluation: 100%"
          }
        },
        "3ec72483447a4662863ce1fa37ab243d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ce059e4491e4f99b1bf8fa3af4f1ce1",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_34b5cf7798cf48e2b49005fc63c9eb53",
            "value": 100
          }
        },
        "3baa14253d684e249a1c50dafd13c7a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26c05cd3d22b4717bc009fe04277b27e",
            "placeholder": "​",
            "style": "IPY_MODEL_7c73098b73b04653bc92421f00db91c8",
            "value": " 100/100 [05:31&lt;00:00,  2.42s/it]"
          }
        },
        "5215a6d2f29240e6a97f945a0d0bc7a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6b6152eda8f41349b7842fc012a7c23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5ebd97b353d43bd8808249a3358b2d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ce059e4491e4f99b1bf8fa3af4f1ce1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34b5cf7798cf48e2b49005fc63c9eb53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "26c05cd3d22b4717bc009fe04277b27e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c73098b73b04653bc92421f00db91c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/comet-ml/opik/main/apps/opik-documentation/documentation/static/img/opik-logo.svg\" width=\"250\"/>"
      ],
      "metadata": {
        "id": "Mm9tmTpAzztM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM AS JUDGES untuk model  (Base & Fine Tune)"
      ],
      "metadata": {
        "id": "fC9ZI9eXWW1E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2RoJE0Fze2G",
        "outputId": "2b994cfe-4222-43c6-fec1-fb28466445f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m885.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.2/565.2 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.1/727.1 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install opik openai comet_ml litellm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opik\n",
        "from opik import Opik, track\n",
        "from opik.evaluation import evaluate\n",
        "from opik.evaluation.metrics import (Hallucination, AnswerRelevance, Moderation, Usefulness)\n",
        "from opik.integrations.openai import track_openai\n",
        "import openai\n",
        "import os\n",
        "from datetime import datetime\n",
        "from getpass import getpass\n",
        "import litellm\n",
        "\n",
        "# Define project name to enable tracing\n",
        "os.environ[\"OPIK_PROJECT_NAME\"] = \"Eval LLM NLP\""
      ],
      "metadata": {
        "id": "DgsM03Idz2wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "icOBBlEh0GZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# opik configs | set di secret colab\n",
        "if \"OPIK_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPIK_API_KEY\"] = userdata.get('OPIK_API_KEY')"
      ],
      "metadata": {
        "id": "dwmtAEP10Gti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workspace = 'final-projek-nlp'"
      ],
      "metadata": {
        "id": "7BXoErhk0NIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opik.configure(workspace= workspace)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv0JESE70I0v",
        "outputId": "51c168b8-0ff2-4e25-8887-feb0d26c0ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Opik is already configured. You can check the settings by viewing the config file at /root/.opik.config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = Opik()"
      ],
      "metadata": {
        "id": "EUoR_zKs0iYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENROUTER_API_KEY'] = userdata.get('OPENROUTER_API_KEY')"
      ],
      "metadata": {
        "id": "Zm_X2rcl0isD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM Client"
      ],
      "metadata": {
        "id": "Ts5AceEMW6L3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMClient:\n",
        "    def __init__(self, client_type: str = \"openai\", model: str = \"gpt-4\"):\n",
        "        self.client_type = client_type\n",
        "        self.model = model\n",
        "\n",
        "        if self.client_type == \"openai\":\n",
        "            self.client = track_openai(openai.OpenAI())\n",
        "        else:  # LiteLLM via OpenRouter\n",
        "            self.client = track_openai(\n",
        "                openai.OpenAI(\n",
        "                    base_url=\"https://openrouter.ai/api/v1\",\n",
        "                    api_key=os.environ[\"OPENROUTER_API_KEY\"]\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def _get_litellm_response(self, query: str, system: str = \"You are a helpful assistant.\"):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": query}\n",
        "        ]\n",
        "        response = litellm.completion(model=self.model, messages=messages)\n",
        "        return response.choices[0].message[\"content\"]\n",
        "\n",
        "    def _get_openai_response(self, query: str, system: str = \"You are a helpful assistant.\", **kwargs):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": query}\n",
        "        ]\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=messages,\n",
        "            **kwargs\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    def query(self, query: str, system: str = \"You are a helpful assistant.\", **kwargs):\n",
        "        if self.client_type == 'openai':\n",
        "            return self._get_openai_response(query, system, **kwargs)\n",
        "        else:\n",
        "            return self._get_litellm_response(query, system)"
      ],
      "metadata": {
        "id": "cvKLqw-60rcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To set clinet\n",
        "MODEL= 'openrouter/deepseek/deepseek-r1' # model bebas milih selama ada di OpenRouter\n",
        "llm_client = LLMClient(model=MODEL, client_type='litellm')"
      ],
      "metadata": {
        "id": "_8isMyos0ye6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_client.query('Pagi')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HKqyVDW8QYi7",
        "outputId": "fc52cc4a-cc59-4a5d-f297-7f66e399fde4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Selamat pagi! Ada yang bisa saya bantu hari ini?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "list-dataset"
      ],
      "metadata": {
        "id": "1GLvZvSY2Lo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[link dataset opik](https://www.comet.com/opik/final-projek-nlp/datasets)"
      ],
      "metadata": {
        "id": "l3nrdwwFXn8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_dataset = ['Model-1 Base | unsloth/mistral-7b-v0.3',\n",
        " 'Model-1 ft | unsloth/mistral-7b-v0.3',\n",
        " 'Model-2 Base | unsloth/mistral-7b-v0.3',\n",
        " 'Model-2 ft | unsloth/mistral-7b-v0.3',\n",
        " 'Model-3 Base | unsloth/mistral-7b-instruct-v0.2-bnb-4bit',\n",
        " 'Model-3 ft | unsloth/mistral-7b-instruct-v0.2-bnb-4bit',\n",
        " 'Model-4 Base | unsloth/llama-3-8b-bnb-4bit',\n",
        " 'Model-4 ft | unsloth/llama-3-8b-bnb-4bit',\n",
        " 'Model-5 Base | GoToCompany/llama3-8b-cpt-sahabatai-v1-instruct',\n",
        " 'Model-5 ft | GoToCompany/llama3-8b-cpt-sahabatai-v1-instruct',\n",
        " 'Model-6 Base | GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct',\n",
        " 'Model-6 ft | GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct',\n",
        " 'Model-7 Base | Yellow-AI-NLP/komodo-7b-base',\n",
        " 'Model-7 ft | Yellow-AI-NLP/komodo-7b-base',\n",
        " 'Model-8 Base | Yellow-AI-NLP/komodo-7b-base',\n",
        " 'Model-8 ft | Yellow-AI-NLP/komodo-7b-base']"
      ],
      "metadata": {
        "id": "y9az1RoN2M8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create or get the dataset\n",
        "# contoh disini karena untuk model 1\n",
        "dataset = client.get_or_create_dataset(name=\"Model-1 Base | unsloth/mistral-7b-v0.3\")"
      ],
      "metadata": {
        "id": "a-rHon3T0zES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make custom model"
      ],
      "metadata": {
        "id": "lKOcEVFmXwny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opik.evaluation.models import OpikBaseModel\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "class CustomModel(OpikBaseModel):\n",
        "    def __init__(self, model_name: str = \"gpt-4\", client_type: str = \"openai\"):\n",
        "        super().__init__(model_name)\n",
        "        self.client = LLMClient(client_type=client_type, model=model_name)\n",
        "\n",
        "    def generate_provider_response(self, messages: List[Dict[str, Any]], **kwargs: Any) -> str:\n",
        "        if not messages or messages[-1][\"role\"] != \"user\":\n",
        "            raise ValueError(\"Last message must be from user.\")\n",
        "        system_message = messages[0][\"content\"] if messages[0][\"role\"] == \"system\" else \"You are a helpful assistant.\"\n",
        "        user_message = messages[-1][\"content\"]\n",
        "        return self.client.query(user_message, system=system_message, **kwargs)\n",
        "\n",
        "    def generate_string(self, input: str, **kwargs: Any) -> str:\n",
        "        return self.client.query(input, **kwargs)"
      ],
      "metadata": {
        "id": "waligIY33MP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_evaluation_task(item):\n",
        "    return {\n",
        "        \"input\": item[\"input\"],\n",
        "        \"output\": item[\"prediction\"],\n",
        "        \"context\": item[\"reference\"]  # reference jadi context untuk hallucination check misal\n",
        "    }"
      ],
      "metadata": {
        "id": "PZ3xeEkd3My1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Custom untuk evaluasi\n",
        "MODEL = 'openrouter/deepseek/deepseek-r1:free'\n",
        "custom_model = CustomModel(model_name=MODEL, client_type=\"litellm\")"
      ],
      "metadata": {
        "id": "zXG8kcZ03Svd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from opik.evaluation.metrics import (Hallucination, AnswerRelevance, Moderation, Usefulness)"
      ],
      "metadata": {
        "id": "szuRm2KW4qd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hallucination metric\n",
        "hallucination_metric = Hallucination(model=custom_model)\n",
        "AnswerRelevanceMetric = AnswerRelevance(model=custom_model)\n",
        "ModerationMetric = Moderation(model=custom_model)\n",
        "UsefulnessMetric = Usefulness(model=custom_model)"
      ],
      "metadata": {
        "id": "e2JqNeXr4E1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nama eksperimen\n",
        "experiment_name = f\"{list_dataset[0]} | {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} | model as judges evaluation: {MODEL}\"\n",
        "experiment_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FhyUlUlR5Aa4",
        "outputId": "63df41da-8ea7-49f3-b6f0-c7bd49d65e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Model-1 Base | unsloth/mistral-7b-v0.3 | 2025-05-31_10-09-42 | model as judges evaluation: openrouter/deepseek/deepseek-r1:free'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation\n",
        "evaluation = evaluate(\n",
        "    experiment_name=experiment_name,\n",
        "    dataset=dataset,\n",
        "    task=simple_evaluation_task,\n",
        "    scoring_metrics=[hallucination_metric, AnswerRelevanceMetric, ModerationMetric, UsefulnessMetric],\n",
        "    experiment_config={\"model\": MODEL,\n",
        "                       \"dataset\" : dataset.name},\n",
        "    task_threads=10,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d104b9ddd845488f9adf66ee2fc4d392",
            "d0406b028c2248549526c5472d3e79c6",
            "685a89fb03e648e18fc0ead76fc5dbd5",
            "e77d727c58f54a99bff29e9ec499cf8c",
            "e2cb4a0a5baf46ddaf9c8433c53db864",
            "ea093126ed644386a6212df8a8ed087e",
            "02e9405a6e674a97b7c2ebf54c88a41f",
            "d6b9bc7f99e14187b8f85424eb48be3d",
            "8b45779bb97d478ba495c34e02277a64",
            "bea5aabb0cc142d595e554554394129d",
            "d2c8307ac52f4025abfa4f90865c5f6f"
          ]
        },
        "collapsed": true,
        "id": "4vdhmVnh5HwF",
        "outputId": "fad97c28-a597-4419-e788-43d972ff9ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation:   0%|          | 0/139 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d104b9ddd845488f9adf66ee2fc4d392"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to parse model output: Failed to extract presumably JSON dictionary: Expecting value: line 1 column 1 (char 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/parsing_helpers.py\", line 22, in _extract_presumably_json_dict_or_raise\n",
            "    return json.loads(json_string)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/__init__.py\", line 346, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/decoder.py\", line 337, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/decoder.py\", line 355, in raw_decode\n",
            "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
            "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/parser.py\", line 11, in parse_model_output\n",
            "    dict_content = parsing_helpers.extract_json_content_or_raise(content)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/parsing_helpers.py\", line 10, in extract_json_content_or_raise\n",
            "    return _extract_presumably_json_dict_or_raise(content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/parsing_helpers.py\", line 24, in _extract_presumably_json_dict_or_raise\n",
            "    raise exceptions.JSONParsingError(\n",
            "opik.exceptions.JSONParsingError: Failed to extract presumably JSON dictionary: Expecting value: line 1 column 1 (char 0)\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/parsing_helpers.py\", line 22, in _extract_presumably_json_dict_or_raise\n",
            "    return json.loads(json_string)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/__init__.py\", line 346, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/decoder.py\", line 337, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/decoder.py\", line 355, in raw_decode\n",
            "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
            "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/parser.py\", line 11, in parse_model_output\n",
            "    dict_content = parsing_helpers.extract_json_content_or_raise(content)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/parsing_helpers.py\", line 10, in extract_json_content_or_raise\n",
            "    return _extract_presumably_json_dict_or_raise(content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/parsing_helpers.py\", line 24, in _extract_presumably_json_dict_or_raise\n",
            "    raise exceptions.JSONParsingError(\n",
            "opik.exceptions.JSONParsingError: Failed to extract presumably JSON dictionary: Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 129, in score\n",
            "    return parser.parse_model_output(content=model_output, name=self.name)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/parser.py\", line 24, in parse_model_output\n",
            "    raise exceptions.MetricComputationError(\n",
            "opik.exceptions.MetricComputationError: Failed to calculate answer relevance score\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662200000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to parse model output: Failed to extract presumably JSON dictionary: Expecting value: line 1 column 1 (char 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/parsing_helpers.py\", line 22, in _extract_presumably_json_dict_or_raise\n",
            "    return json.loads(json_string)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/__init__.py\", line 346, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/decoder.py\", line 337, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/decoder.py\", line 355, in raw_decode\n",
            "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
            "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/parser.py\", line 11, in parse_model_output\n",
            "    dict_content = parsing_helpers.extract_json_content_or_raise(content)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/parsing_helpers.py\", line 10, in extract_json_content_or_raise\n",
            "    return _extract_presumably_json_dict_or_raise(content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/parsing_helpers.py\", line 24, in _extract_presumably_json_dict_or_raise\n",
            "    raise exceptions.JSONParsingError(\n",
            "opik.exceptions.JSONParsingError: Failed to extract presumably JSON dictionary: Expecting value: line 1 column 1 (char 0)\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/parsing_helpers.py\", line 22, in _extract_presumably_json_dict_or_raise\n",
            "    return json.loads(json_string)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/__init__.py\", line 346, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/decoder.py\", line 337, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/decoder.py\", line 355, in raw_decode\n",
            "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
            "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/parser.py\", line 11, in parse_model_output\n",
            "    dict_content = parsing_helpers.extract_json_content_or_raise(content)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/parsing_helpers.py\", line 10, in extract_json_content_or_raise\n",
            "    return _extract_presumably_json_dict_or_raise(content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/parsing_helpers.py\", line 24, in _extract_presumably_json_dict_or_raise\n",
            "    raise exceptions.JSONParsingError(\n",
            "opik.exceptions.JSONParsingError: Failed to extract presumably JSON dictionary: Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 95, in score\n",
            "    return parser.parse_model_output(content=model_output, name=self.name)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/parser.py\", line 26, in parse_model_output\n",
            "    raise exceptions.MetricComputationError(\n",
            "opik.exceptions.MetricComputationError: Failed hallucination detection\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662260000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to parse model output: Failed to extract presumably JSON dictionary: Expecting value: line 1 column 1 (char 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/parsing_helpers.py\", line 22, in _extract_presumably_json_dict_or_raise\n",
            "    return json.loads(json_string)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/__init__.py\", line 346, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/decoder.py\", line 337, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/decoder.py\", line 355, in raw_decode\n",
            "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
            "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/parser.py\", line 11, in parse_model_output\n",
            "    dict_content = parsing_helpers.extract_json_content_or_raise(content)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/parsing_helpers.py\", line 10, in extract_json_content_or_raise\n",
            "    return _extract_presumably_json_dict_or_raise(content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/parsing_helpers.py\", line 24, in _extract_presumably_json_dict_or_raise\n",
            "    raise exceptions.JSONParsingError(\n",
            "opik.exceptions.JSONParsingError: Failed to extract presumably JSON dictionary: Expecting value: line 1 column 1 (char 0)\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/parsing_helpers.py\", line 22, in _extract_presumably_json_dict_or_raise\n",
            "    return json.loads(json_string)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/__init__.py\", line 346, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/decoder.py\", line 337, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/decoder.py\", line 355, in raw_decode\n",
            "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
            "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/parser.py\", line 11, in parse_model_output\n",
            "    dict_content = parsing_helpers.extract_json_content_or_raise(content)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/parsing_helpers.py\", line 10, in extract_json_content_or_raise\n",
            "    return _extract_presumably_json_dict_or_raise(content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/parsing_helpers.py\", line 24, in _extract_presumably_json_dict_or_raise\n",
            "    raise exceptions.JSONParsingError(\n",
            "opik.exceptions.JSONParsingError: Failed to extract presumably JSON dictionary: Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 129, in score\n",
            "    return parser.parse_model_output(content=model_output, name=self.name)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/parser.py\", line 24, in parse_model_output\n",
            "    raise exceptions.MetricComputationError(\n",
            "opik.exceptions.MetricComputationError: Failed to calculate answer relevance score\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n",
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2145, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-min. \",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"20\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1748662320000\"},\"provider_name\":null}},\"user_id\":\"user_2p1Pw7VZBtsBz0IdBJCDdpiiL16\"}\n",
            "OPIK: LLM provider rate limit error detected. We recommend reducing the amount of parallel requests by setting `task_threads` evaluation parameter to a smaller number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/evaluation_tasks_executor.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(evaluation_tasks, workers, verbose)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         test_results = [\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mtest_result_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/evaluation_tasks_executor.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         test_results = [\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mtest_result_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-5956c2febb97>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m evaluation = evaluate(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimple_evaluation_task\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/opik/evaluation/evaluator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(dataset, task, scoring_metrics, experiment_name, project_name, experiment_config, verbose, nb_samples, task_threads, prompt, prompts, scoring_key_mapping, dataset_item_ids)\u001b[0m\n\u001b[1;32m     92\u001b[0m     )\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     return _evaluate_task(\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mexperiment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/opik/evaluation/evaluator.py\u001b[0m in \u001b[0;36m_evaluate_task\u001b[0;34m(client, experiment, dataset, task, scoring_metrics, project_name, verbose, nb_samples, task_threads, scoring_key_mapping, dataset_item_ids)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mscoring_key_mapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring_key_mapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         )\n\u001b[0;32m--> 135\u001b[0;31m         test_results = evaluation_engine.evaluate_llm_tasks(\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0mdataset_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\u001b[0m in \u001b[0;36mevaluate_llm_tasks\u001b[0;34m(self, dataset_, task, nb_samples, dataset_item_ids)\u001b[0m\n\u001b[1;32m    174\u001b[0m         ]\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         test_results = evaluation_tasks_executor.execute(\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0mevaluation_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/evaluation_tasks_executor.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(evaluation_tasks, workers, verbose)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtest_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         test_result_futures = [\n\u001b[1;32m     29\u001b[0m             \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_task\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevaluation_task\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevaluation_tasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Nama eksperimen\n",
        "\n",
        "MODEL = 'openrouter/openai/gpt-4o-mini'\n",
        "custom_model = CustomModel(model_name=MODEL, client_type=\"litellm\")\n",
        "\n",
        "experiment_name = f\"{list_dataset[0]} | {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} | model as judges evaluation: {MODEL}\"\n",
        "experiment_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kZtlLAnDbcos",
        "outputId": "2ab41880-f89d-408f-a776-e03e3e710fc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Model-1 Base | unsloth/mistral-7b-v0.3 | 2025-05-31_03-42-25 | model as judges evaluation: openrouter/openai/gpt-4o-mini'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function for evaluation"
      ],
      "metadata": {
        "id": "ClimI4NN0KJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def run_evaluation(model_name, dataset,no_dataset, thread, sample= None):\n",
        "    custom_model = CustomModel(model_name=model_name, client_type=\"litellm\")\n",
        "\n",
        "    # Metrik evaluasi\n",
        "    hallucination = Hallucination(model=custom_model)\n",
        "    relevance = AnswerRelevance(model=custom_model)\n",
        "    moderation = Moderation(model=custom_model)\n",
        "    usefulness = Usefulness(model=custom_model)\n",
        "\n",
        "    # Nama eksperimen\n",
        "    experiment_name = f\"{list_dataset[no_dataset]} | {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} | model as judges evaluation: {model_name}\"\n",
        "\n",
        "    # Jalankan evaluasi\n",
        "    evaluation = evaluate(\n",
        "        experiment_name=experiment_name,\n",
        "        dataset=dataset,\n",
        "        task=simple_evaluation_task,\n",
        "        scoring_metrics=[hallucination, relevance, moderation, usefulness],\n",
        "        experiment_config={\"model\": model_name, \"dataset\": dataset.name},\n",
        "        task_threads=thread,\n",
        "        nb_samples=sample\n",
        "    )\n",
        "\n",
        "    return evaluation"
      ],
      "metadata": {
        "id": "RNclcXeygQPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1"
      ],
      "metadata": {
        "id": "8O0u3UxqxGlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1 Base"
      ],
      "metadata": {
        "id": "POTw9cy5vaYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'openrouter/openai/gpt-4o-mini'\n",
        "dataset_no = 0 # index\n",
        "task_thread = 4\n",
        "sample = None\n",
        "dataset = client.get_or_create_dataset(name=list_dataset[dataset_no])\n",
        "result = run_evaluation(MODEL, dataset, dataset_no, task_thread, sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261,
          "referenced_widgets": [
            "4a1a11805c6c46429156045e77d9a6e8",
            "7499ad6d96ef477aaaa5c37e3d13db20",
            "7ba5d1a92969426991d45530202ad424",
            "c29f0cced4734558bd9c234fb4de2043",
            "0187071be4f94727925d2d07a360fc9d",
            "7c10e4e60c5a434ba2decad2e5a30f09",
            "836f20e3643545baa3af82e0a6ccb5a9",
            "71a39dcae0cb48e3a095a39c335555ae",
            "bdfa53cb79fd49b0b539af65f074651f",
            "34fa9642fa6648b3bd2de5d9dd00f154",
            "8308a9461dcc42e4a6d24e96a27e7a21"
          ]
        },
        "id": "ax4A7pnAgh6-",
        "outputId": "d8fd0cd2-6e19-47fe-db33-be4f135c96f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation:   0%|          | 0/139 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a1a11805c6c46429156045e77d9a6e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ Model-1 Base | unsloth/mistral-7b-v0.3 (139 samples) ─╮\n",
              "│                                                        │\n",
              "│ \u001b[1mTotal time:       \u001b[0m 00:04:49                            │\n",
              "│ \u001b[1mNumber of samples:\u001b[0m 139                                 │\n",
              "│                                                        │\n",
              "│ \u001b[1;32mhallucination_metric: 0.7525 (avg)\u001b[0m                     │\n",
              "│ \u001b[1;32manswer_relevance_metric: 0.4665 (avg)\u001b[0m                  │\n",
              "│ \u001b[1;32mmoderation_metric: 0.0230 (avg)\u001b[0m                        │\n",
              "│ \u001b[1;32mUsefulnessMetric: 0.2561 (avg)\u001b[0m                         │\n",
              "│                                                        │\n",
              "╰────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ Model-1 Base | unsloth/mistral-7b-v0.3 (139 samples) ─╮\n",
              "│                                                        │\n",
              "│ <span style=\"font-weight: bold\">Total time:       </span> 00:04:49                            │\n",
              "│ <span style=\"font-weight: bold\">Number of samples:</span> 139                                 │\n",
              "│                                                        │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">hallucination_metric: 0.7525 (avg)</span>                     │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">answer_relevance_metric: 0.4665 (avg)</span>                  │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">moderation_metric: 0.0230 (avg)</span>                        │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">UsefulnessMetric: 0.2561 (avg)</span>                         │\n",
              "│                                                        │\n",
              "╰────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading results to Opik \u001b[33m...\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "View the results \u001b]8;id=422756;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=01972484-ad58-7f93-87b0-5c405db20ba5&dataset_id=019723cb-8132-7b6c-8a09-5e865fd216c6&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=01972484-ad58-7f93-87b0-5c405db20ba5&dataset_id=019723cb-8132-7b6c-8a09-5e865fd216c6&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model  1 FT"
      ],
      "metadata": {
        "id": "Fj7Clh7mkpOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'openrouter/openai/gpt-4o-mini'\n",
        "dataset_no = 1 # index\n",
        "task_thread = 4\n",
        "sample = None\n",
        "dataset = client.get_or_create_dataset(name=list_dataset[dataset_no])\n",
        "result = run_evaluation(MODEL, dataset, dataset_no, task_thread, sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3dbfeea9f13d46929862420652999b17",
            "06a90c6c8b5e4153abf4b6c4e4f24e1c",
            "e956672642e54fefa31be8b4c6ecf839",
            "ffc1d54e95e44aea9a323a2ed29f8976",
            "a0adc22938a04c8fa26e8c0330aaf1d0",
            "514cc126d9b143f89123a60c16ebb4d2",
            "b148b6b2425c4251923057676135f803",
            "cecdd04f9b7a4d96acfd8a5a1026c233",
            "0bf6b793a162483fa0c7f38aae129685",
            "e9dfd14099da42f0b72484050525079f",
            "f7c7bc2666ee4b61ad34bef0cca56fd3"
          ]
        },
        "id": "eVSKHZ1ukRRy",
        "outputId": "7b7b0b19-c7c8-4150-8f1f-46681421ee04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation:   0%|          | 0/148 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3dbfeea9f13d46929862420652999b17"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2172, in exception_type\n",
            "    raise APIError(\n",
            "litellm.exceptions.APIError: litellm.APIError: APIError: OpenrouterException - {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ Model-1 ft | unsloth/mistral-7b-v0.3 (148 samples) ─╮\n",
              "│                                                      │\n",
              "│ \u001b[1mTotal time:       \u001b[0m 00:05:58                          │\n",
              "│ \u001b[1mNumber of samples:\u001b[0m 148                               │\n",
              "│                                                      │\n",
              "│ \u001b[1;32mhallucination_metric: 0.5020 (avg)\u001b[0m                   │\n",
              "│ \u001b[1;32manswer_relevance_metric: 0.8291 (avg)\u001b[0m                │\n",
              "│ \u001b[1;32mmoderation_metric: 0.0020 (avg)\u001b[0m                      │\n",
              "│ \u001b[1;32mUsefulnessMetric: 0.6871 (avg)\u001b[0m\u001b[31m - 1 failed\u001b[0m            │\n",
              "│                                                      │\n",
              "╰──────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ Model-1 ft | unsloth/mistral-7b-v0.3 (148 samples) ─╮\n",
              "│                                                      │\n",
              "│ <span style=\"font-weight: bold\">Total time:       </span> 00:05:58                          │\n",
              "│ <span style=\"font-weight: bold\">Number of samples:</span> 148                               │\n",
              "│                                                      │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">hallucination_metric: 0.5020 (avg)</span>                   │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">answer_relevance_metric: 0.8291 (avg)</span>                │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">moderation_metric: 0.0020 (avg)</span>                      │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">UsefulnessMetric: 0.6871 (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 1 failed</span>            │\n",
              "│                                                      │\n",
              "╰──────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading results to Opik \u001b[33m...\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "View the results \u001b]8;id=188937;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019724a7-da3c-7662-9d3f-882ddfb4b6ec&dataset_id=019723cb-82d2-751c-86cb-ef9e0213ec6b&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019724a7-da3c-7662-9d3f-882ddfb4b6ec&dataset_id=019723cb-82d2-751c-86cb-ef9e0213ec6b&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2"
      ],
      "metadata": {
        "id": "FkArv31vxLtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2 Base"
      ],
      "metadata": {
        "id": "F45QNIfWvyMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'openrouter/openai/gpt-4o-mini'\n",
        "dataset_no = 2 # index\n",
        "task_thread = 4\n",
        "sample = None\n",
        "print('model:', list_dataset[dataset_no])\n",
        "dataset = client.get_or_create_dataset(name=list_dataset[dataset_no])\n",
        "result = run_evaluation(MODEL, dataset, dataset_no, task_thread, sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252,
          "referenced_widgets": [
            "08de8bdfacc34c0bb0f106e220ecb296",
            "eb7d39c2963f498caeff6ecd1199a619",
            "de4d019568ce4917ab6b0418db02cbef",
            "4bfed8c41b834c259b9b9be3427d8e89",
            "95e509460e2a4ce885a512a53266996a",
            "9acfb0fd62ee42fbb27ceb356ab4b5d2",
            "d049a99430e843de85db9d46bf9ea149",
            "545788f9c285406a9e472bd5fe6b999d",
            "cd3d034908794415ae1b54311354ba86",
            "f5935bd77e8a48288325db3d7626b450",
            "fa1c2e27689b4b2e95d13595012b9c4b"
          ]
        },
        "id": "DwvhN2wJvt2_",
        "outputId": "ca491d47-310f-4bc9-dbcb-c0ed1294e504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: Model-2 Base | unsloth/mistral-7b-v0.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Created a \"Model-2 Base | unsloth/mistral-7b-v0.3\" dataset at https://www.comet.com/opik/api/v1/session/redirect/datasets/?dataset_id=019724c2-c823-767c-bce4-7b5f7eefba53&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08de8bdfacc34c0bb0f106e220ecb296"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ Model-2 Base | unsloth/mistral-7b-v0.3 (0 samples) ─╮\n",
              "│                                                      │\n",
              "│ \u001b[1mTotal time:       \u001b[0m 00:00:00                          │\n",
              "│ \u001b[1mNumber of samples:\u001b[0m 0                                 │\n",
              "│                                                      │\n",
              "│                                                      │\n",
              "╰──────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ Model-2 Base | unsloth/mistral-7b-v0.3 (0 samples) ─╮\n",
              "│                                                      │\n",
              "│ <span style=\"font-weight: bold\">Total time:       </span> 00:00:00                          │\n",
              "│ <span style=\"font-weight: bold\">Number of samples:</span> 0                                 │\n",
              "│                                                      │\n",
              "│                                                      │\n",
              "╰──────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading results to Opik \u001b[33m...\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "View the results \u001b]8;id=233328;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019724c2-c8d4-76ec-87f8-16501f101225&dataset_id=019724c2-c823-767c-bce4-7b5f7eefba53&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019724c2-c8d4-76ec-87f8-16501f101225&dataset_id=019724c2-c823-767c-bce4-7b5f7eefba53&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model 2 ft"
      ],
      "metadata": {
        "id": "jcFPHRvHwuj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'openrouter/openai/gpt-4o-mini'\n",
        "dataset_no = 3 # index\n",
        "task_thread = 4\n",
        "sample = None\n",
        "print('model:', list_dataset[dataset_no])\n",
        "dataset = client.get_or_create_dataset(name=list_dataset[dataset_no])\n",
        "result = run_evaluation(MODEL, dataset, dataset_no, task_thread, sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252,
          "referenced_widgets": [
            "67f6ed09ab9a4d1cba57bc558bd9c3e2",
            "b4e11f6c3f45464ebb11a85b9f3f6aec",
            "444d1d2228094774b53618e833673441",
            "b20275da1c0f4902953f1a5e7b358489",
            "d058d8e588b449cc90df1de29e3396ca",
            "445a99cf5d0046b4a5bb1388cdbaf7a9",
            "36411d5c8f454b93aca7c1f2ac5d6efb",
            "c76152a36c564709a76e1b60385bb152",
            "453bafbb626e48d48f1f91bd25ca8a53",
            "376a49566e2e4d9d95ee274b12ec7a33",
            "7588ec39b4334aa1abefbb5dbdcadeb5"
          ]
        },
        "id": "XEJQfcDcwuCA",
        "outputId": "96b55cb8-6c87-414d-f386-8ed730dd6324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: Model-2 ft | unsloth/mistral-7b-v0.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Created a \"Model-2 ft | unsloth/mistral-7b-v0.3\" dataset at https://www.comet.com/opik/api/v1/session/redirect/datasets/?dataset_id=019724c3-29e4-7f7f-87a3-047609d2fe07&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67f6ed09ab9a4d1cba57bc558bd9c3e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ Model-2 ft | unsloth/mistral-7b-v0.3 (0 samples) ─╮\n",
              "│                                                    │\n",
              "│ \u001b[1mTotal time:       \u001b[0m 00:00:00                        │\n",
              "│ \u001b[1mNumber of samples:\u001b[0m 0                               │\n",
              "│                                                    │\n",
              "│                                                    │\n",
              "╰────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ Model-2 ft | unsloth/mistral-7b-v0.3 (0 samples) ─╮\n",
              "│                                                    │\n",
              "│ <span style=\"font-weight: bold\">Total time:       </span> 00:00:00                        │\n",
              "│ <span style=\"font-weight: bold\">Number of samples:</span> 0                               │\n",
              "│                                                    │\n",
              "│                                                    │\n",
              "╰────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading results to Opik \u001b[33m...\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "View the results \u001b]8;id=463521;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019724c3-2ad2-72e5-9878-146d034d9729&dataset_id=019724c3-29e4-7f7f-87a3-047609d2fe07&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019724c3-2ad2-72e5-9878-146d034d9729&dataset_id=019724c3-29e4-7f7f-87a3-047609d2fe07&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 3"
      ],
      "metadata": {
        "id": "D--pSRygxP-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model 3 base"
      ],
      "metadata": {
        "id": "H3VOW1Trw4y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'openrouter/openai/gpt-4o-mini'\n",
        "dataset_no = 4 # index\n",
        "task_thread = 4\n",
        "sample = None\n",
        "print('model:', list_dataset[dataset_no])\n",
        "dataset = client.get_or_create_dataset(name=list_dataset[dataset_no])\n",
        "result = run_evaluation(MODEL, dataset, dataset_no, task_thread, sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279,
          "referenced_widgets": [
            "6faf2c1a382f4201a819ee6342a1fd3f",
            "6d79932ad83f47caaa60520457a549b6",
            "1313bf7505f34e1e848f5aa44f75732c",
            "17dfeb5971ed44aa8ce04267d1d30248",
            "4fe5db0d59994d7aabe8c5a6cd1194ea",
            "c0b8ade8311c4a5ea59ae6ccee06c2e1",
            "aa166e5985de4fbda3dedd8dee9a6a9d",
            "cbaf787b057f4e1091781e6dd05d3a1d",
            "3bce94f3764b43bb87e49ecacf7a0dcb",
            "81857923a73f472499def6f33b4a5b70",
            "04a27c01cdd341b2888ea91ee89d766f"
          ]
        },
        "id": "uITemoSXwyhe",
        "outputId": "b56c1f78-f76d-4253-b8a9-8a4c9be934f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: Model-3 Base | unsloth/mistral-7b-instruct-v0.2-bnb-4bit\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation:   0%|          | 0/101 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6faf2c1a382f4201a819ee6342a1fd3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ Model-3 Base | unsloth/mistral-7b-instruct-v0.2-bnb-4bit (101 samples) ─╮\n",
              "│                                                                          │\n",
              "│ \u001b[1mTotal time:       \u001b[0m 00:04:37                                              │\n",
              "│ \u001b[1mNumber of samples:\u001b[0m 101                                                   │\n",
              "│                                                                          │\n",
              "│ \u001b[1;32mhallucination_metric: 0.7069 (avg)\u001b[0m                                       │\n",
              "│ \u001b[1;32manswer_relevance_metric: 0.7718 (avg)\u001b[0m                                    │\n",
              "│ \u001b[1;32mmoderation_metric: 0.0010 (avg)\u001b[0m                                          │\n",
              "│ \u001b[1;32mUsefulnessMetric: 0.6257 (avg)\u001b[0m                                           │\n",
              "│                                                                          │\n",
              "╰──────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ Model-3 Base | unsloth/mistral-7b-instruct-v0.2-bnb-4bit (101 samples) ─╮\n",
              "│                                                                          │\n",
              "│ <span style=\"font-weight: bold\">Total time:       </span> 00:04:37                                              │\n",
              "│ <span style=\"font-weight: bold\">Number of samples:</span> 101                                                   │\n",
              "│                                                                          │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">hallucination_metric: 0.7069 (avg)</span>                                       │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">answer_relevance_metric: 0.7718 (avg)</span>                                    │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">moderation_metric: 0.0010 (avg)</span>                                          │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">UsefulnessMetric: 0.6257 (avg)</span>                                           │\n",
              "│                                                                          │\n",
              "╰──────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading results to Opik \u001b[33m...\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "View the results \u001b]8;id=9135;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019724c3-cef9-75c3-be29-7dd504e2f6ef&dataset_id=019723cb-84c9-7023-8a99-5a1392ce82b3&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019724c3-cef9-75c3-be29-7dd504e2f6ef&dataset_id=019723cb-84c9-7023-8a99-5a1392ce82b3&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model 3 ft"
      ],
      "metadata": {
        "id": "gHFaelg63n0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'openrouter/openai/gpt-4o-mini'\n",
        "dataset_no = 5 # index\n",
        "task_thread = 4\n",
        "sample = None\n",
        "print('model:', list_dataset[dataset_no])\n",
        "dataset = client.get_or_create_dataset(name=list_dataset[dataset_no])\n",
        "result = run_evaluation(MODEL, dataset, dataset_no, task_thread, sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "90e9acba5bec4701872437fc7e425caa",
            "9914b0133d954a24a85428a26f6df09c",
            "4e957f5441584b599c6e6d3f17dbcfbf",
            "f47d2a5d6ba7417f80f4b253c2e9819c",
            "e6351dd6bde24cf7975e04389469f5b3",
            "64fb8783645b497ea6f56a9848e826b2",
            "245e2beca3204261a0ff6070b5ff0936",
            "3515364d471d4473a50a87c94103a442",
            "7a9efc4888cd4b5da7ae5d0be6df7171",
            "66053092ffb043489ccb37c7ddfb8913",
            "caae8be46e7f4f639a11ec83eead7732"
          ]
        },
        "id": "q_UqFVcK3ovd",
        "outputId": "f4210d21-2ac2-4d26-b152-bb4e30789a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: Model-3 ft | unsloth/mistral-7b-instruct-v0.2-bnb-4bit\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90e9acba5bec4701872437fc7e425caa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2172, in exception_type\n",
            "    raise APIError(\n",
            "litellm.exceptions.APIError: litellm.APIError: APIError: OpenrouterException - {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ Model-3 ft | unsloth/mistral-7b-instruct-v0.2-bnb-4bit (100 samples) ─╮\n",
              "│                                                                        │\n",
              "│ \u001b[1mTotal time:       \u001b[0m 00:04:13                                            │\n",
              "│ \u001b[1mNumber of samples:\u001b[0m 100                                                 │\n",
              "│                                                                        │\n",
              "│ \u001b[1;32mhallucination_metric: 0.4870 (avg)\u001b[0m                                     │\n",
              "│ \u001b[1;32manswer_relevance_metric: 0.8265 (avg)\u001b[0m                                  │\n",
              "│ \u001b[1;32mmoderation_metric: 0.0080 (avg)\u001b[0m                                        │\n",
              "│ \u001b[1;32mUsefulnessMetric: 0.7051 (avg)\u001b[0m\u001b[31m - 1 failed\u001b[0m                              │\n",
              "│                                                                        │\n",
              "╰────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ Model-3 ft | unsloth/mistral-7b-instruct-v0.2-bnb-4bit (100 samples) ─╮\n",
              "│                                                                        │\n",
              "│ <span style=\"font-weight: bold\">Total time:       </span> 00:04:13                                            │\n",
              "│ <span style=\"font-weight: bold\">Number of samples:</span> 100                                                 │\n",
              "│                                                                        │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">hallucination_metric: 0.4870 (avg)</span>                                     │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">answer_relevance_metric: 0.8265 (avg)</span>                                  │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">moderation_metric: 0.0080 (avg)</span>                                        │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">UsefulnessMetric: 0.7051 (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 1 failed</span>                              │\n",
              "│                                                                        │\n",
              "╰────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading results to Opik \u001b[33m...\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "View the results \u001b]8;id=790136;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019724de-b534-7515-ae0a-3705d1a9d4d0&dataset_id=019723cb-865a-719c-9222-f56760c6134f&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019724de-b534-7515-ae0a-3705d1a9d4d0&dataset_id=019723cb-865a-719c-9222-f56760c6134f&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 4"
      ],
      "metadata": {
        "id": "Slz5_KXXxTvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model 4 base"
      ],
      "metadata": {
        "id": "JzNSxIm05HC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'openrouter/openai/gpt-4o-mini'\n",
        "dataset_no = 6 # index\n",
        "task_thread = 4\n",
        "sample = None\n",
        "print('model:', list_dataset[dataset_no])\n",
        "dataset = client.get_or_create_dataset(name=list_dataset[dataset_no])\n",
        "result = run_evaluation(MODEL, dataset, dataset_no, task_thread, sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279,
          "referenced_widgets": [
            "aa9064e7470043e7a600250541c02845",
            "7d7426f2d8df4aa1b24e96efeb28e40c",
            "ddd7cbf268794600865ae5e06d606e09",
            "5cba9496b49740a5b789123aac39e9ca",
            "34b2f919efe64b8fb5e7b03fe04abfff",
            "c7d8016b062941bf9353bda0cb14b88c",
            "0d4d1ab282034baa93581fecfdb9f861",
            "9ef6549244ee4c8fb5940c749ef5d525",
            "ae44c0b70d1542b5b6fa83a62da7358d",
            "cc2975b0a5904fefa3ce2552df1e09b0",
            "d19070d5716c4fa383414bfe65e0318b"
          ]
        },
        "id": "Go1Acx-Q5Hui",
        "outputId": "74006f20-344c-434e-97bd-af2ff59a3b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: Model-4 Base | unsloth/llama-3-8b-bnb-4bit\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa9064e7470043e7a600250541c02845"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ Model-4 Base | unsloth/llama-3-8b-bnb-4bit (100 samples) ─╮\n",
              "│                                                            │\n",
              "│ \u001b[1mTotal time:       \u001b[0m 00:03:56                                │\n",
              "│ \u001b[1mNumber of samples:\u001b[0m 100                                     │\n",
              "│                                                            │\n",
              "│ \u001b[1;32mhallucination_metric: 0.7510 (avg)\u001b[0m                         │\n",
              "│ \u001b[1;32manswer_relevance_metric: 0.5640 (avg)\u001b[0m                      │\n",
              "│ \u001b[1;32mmoderation_metric: 0.0180 (avg)\u001b[0m                            │\n",
              "│ \u001b[1;32mUsefulnessMetric: 0.3400 (avg)\u001b[0m                             │\n",
              "│                                                            │\n",
              "╰────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ Model-4 Base | unsloth/llama-3-8b-bnb-4bit (100 samples) ─╮\n",
              "│                                                            │\n",
              "│ <span style=\"font-weight: bold\">Total time:       </span> 00:03:56                                │\n",
              "│ <span style=\"font-weight: bold\">Number of samples:</span> 100                                     │\n",
              "│                                                            │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">hallucination_metric: 0.7510 (avg)</span>                         │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">answer_relevance_metric: 0.5640 (avg)</span>                      │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">moderation_metric: 0.0180 (avg)</span>                            │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">UsefulnessMetric: 0.3400 (avg)</span>                             │\n",
              "│                                                            │\n",
              "╰────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading results to Opik \u001b[33m...\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "View the results \u001b]8;id=652055;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=01972504-012a-7e31-90e7-a2f204d35149&dataset_id=01972501-ff70-76e6-b842-1495d1994c85&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=01972504-012a-7e31-90e7-a2f204d35149&dataset_id=01972501-ff70-76e6-b842-1495d1994c85&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model 4 ft"
      ],
      "metadata": {
        "id": "aaWq8g-J6jbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'openrouter/openai/gpt-4o-mini'\n",
        "dataset_no = 7 # index\n",
        "task_thread = 4\n",
        "sample = None\n",
        "print('model:', list_dataset[dataset_no])\n",
        "dataset = client.get_or_create_dataset(name=list_dataset[dataset_no])\n",
        "result = run_evaluation(MODEL, dataset, dataset_no, task_thread, sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ba2e73cf373d4925be71ace81ab9004e",
            "6146d7e2aa9c4ccbab229478016d9fd4",
            "d734b8fab0194956bf567a34bbfd1839",
            "eb3e36873440484590d24bd49b13fdda",
            "bdd9c6d6e761480c85a265d0d3a14a48",
            "00ebfe6b519e468298a97cf2e15ea85a",
            "2546662abbc940c8b3e94b7226ba3d45",
            "e720acb49b214318b19c5da48dc9a171",
            "17f7c2f75d794ded9c2d03b938164829",
            "2a0b3ce4b63c49fba93044234a58a9e0",
            "fe80ae0d76144be3a8463c8e6325bb9f"
          ]
        },
        "id": "2UGev39x6gMT",
        "outputId": "d2866cd2-1f4c-4d49-a725-ffb2e43f4852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: Model-4 ft | unsloth/llama-3-8b-bnb-4bit\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba2e73cf373d4925be71ace81ab9004e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2172, in exception_type\n",
            "    raise APIError(\n",
            "litellm.exceptions.APIError: litellm.APIError: APIError: OpenrouterException - {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ Model-4 ft | unsloth/llama-3-8b-bnb-4bit (100 samples) ─╮\n",
              "│                                                          │\n",
              "│ \u001b[1mTotal time:       \u001b[0m 00:03:50                              │\n",
              "│ \u001b[1mNumber of samples:\u001b[0m 100                                   │\n",
              "│                                                          │\n",
              "│ \u001b[1;32mhallucination_metric: 0.4820 (avg)\u001b[0m                       │\n",
              "│ \u001b[1;32manswer_relevance_metric: 0.8320 (avg)\u001b[0m                    │\n",
              "│ \u001b[1;32mmoderation_metric: 0.0050 (avg)\u001b[0m                          │\n",
              "│ \u001b[1;32mUsefulnessMetric: 0.6990 (avg)\u001b[0m\u001b[31m - 1 failed\u001b[0m                │\n",
              "│                                                          │\n",
              "╰──────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ Model-4 ft | unsloth/llama-3-8b-bnb-4bit (100 samples) ─╮\n",
              "│                                                          │\n",
              "│ <span style=\"font-weight: bold\">Total time:       </span> 00:03:50                              │\n",
              "│ <span style=\"font-weight: bold\">Number of samples:</span> 100                                   │\n",
              "│                                                          │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">hallucination_metric: 0.4820 (avg)</span>                       │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">answer_relevance_metric: 0.8320 (avg)</span>                    │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">moderation_metric: 0.0050 (avg)</span>                          │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">UsefulnessMetric: 0.6990 (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 1 failed</span>                │\n",
              "│                                                          │\n",
              "╰──────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading results to Opik \u001b[33m...\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "View the results \u001b]8;id=533910;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019724ea-6705-713a-a63b-7111d64181be&dataset_id=019723cb-8908-7995-9572-4721a6ca30a4&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019724ea-6705-713a-a63b-7111d64181be&dataset_id=019723cb-8908-7995-9572-4721a6ca30a4&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 5"
      ],
      "metadata": {
        "id": "IXtyBKPbxXig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model base 5"
      ],
      "metadata": {
        "id": "-NiimKZh9uKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'openrouter/openai/gpt-4o-mini'\n",
        "dataset_no = 8 # index\n",
        "task_thread = 4\n",
        "sample = None\n",
        "print('model:', list_dataset[dataset_no])\n",
        "dataset = client.get_or_create_dataset(name=list_dataset[dataset_no])\n",
        "result = run_evaluation(MODEL, dataset, dataset_no, task_thread, sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9123bdf6fd2a4f6499ebaad623599525",
            "31284da60c4d489c8c2553b2906e314b",
            "332d3491e0db4d44a23af7804da9ab27",
            "401e8f803a024a4e9964cf6326a00801",
            "8b13e8c56078400b821f4f87ae9dea15",
            "e2efd647e822427aac87784e4cf25951",
            "0275cb7f4ee1439d93abde6247bd129a",
            "f8908f862324402ca0f13220c8939ea1",
            "341df57755ab4858916d09d04c38b479",
            "0da5d2f1e5924519ba4f47f9240e1948",
            "32540666c2c04146bfbc8d5da9e09b1c"
          ]
        },
        "id": "6i8Ng3XV9ta4",
        "outputId": "02abc660-8548-4bac-cb1c-a6db69de988b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: Model-5 Base | GoToCompany/llama3-8b-cpt-sahabatai-v1-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation:   0%|          | 0/105 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9123bdf6fd2a4f6499ebaad623599525"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-30-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2172, in exception_type\n",
            "    raise APIError(\n",
            "litellm.exceptions.APIError: litellm.APIError: APIError: OpenrouterException - {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-30-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2172, in exception_type\n",
            "    raise APIError(\n",
            "litellm.exceptions.APIError: litellm.APIError: APIError: OpenrouterException - {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ Model-5 Base | GoToCompany/llama3-8b-cpt-sahabatai-v1-instruct (105 samples) ─╮\n",
              "│                                                                                │\n",
              "│ \u001b[1mTotal time:       \u001b[0m 00:05:47                                                    │\n",
              "│ \u001b[1mNumber of samples:\u001b[0m 105                                                         │\n",
              "│                                                                                │\n",
              "│ \u001b[1;32mhallucination_metric: 0.4171 (avg)\u001b[0m                                             │\n",
              "│ \u001b[1;32manswer_relevance_metric: 0.8543 (avg)\u001b[0m                                          │\n",
              "│ \u001b[1;32mmoderation_metric: 0.0019 (avg)\u001b[0m\u001b[31m - 1 failed\u001b[0m                                     │\n",
              "│ \u001b[1;32mUsefulnessMetric: 0.7327 (avg)\u001b[0m\u001b[31m - 1 failed\u001b[0m                                      │\n",
              "│                                                                                │\n",
              "╰────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ Model-5 Base | GoToCompany/llama3-8b-cpt-sahabatai-v1-instruct (105 samples) ─╮\n",
              "│                                                                                │\n",
              "│ <span style=\"font-weight: bold\">Total time:       </span> 00:05:47                                                    │\n",
              "│ <span style=\"font-weight: bold\">Number of samples:</span> 105                                                         │\n",
              "│                                                                                │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">hallucination_metric: 0.4171 (avg)</span>                                             │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">answer_relevance_metric: 0.8543 (avg)</span>                                          │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">moderation_metric: 0.0019 (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 1 failed</span>                                     │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">UsefulnessMetric: 0.7327 (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 1 failed</span>                                      │\n",
              "│                                                                                │\n",
              "╰────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading results to Opik \u001b[33m...\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "View the results \u001b]8;id=41661;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=01972622-f198-76bf-80fc-690f05593d43&dataset_id=019723cb-8a7a-7716-8fec-61cb1bd84e64&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=01972622-f198-76bf-80fc-690f05593d43&dataset_id=019723cb-8a7a-7716-8fec-61cb1bd84e64&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model 5 ft"
      ],
      "metadata": {
        "id": "8AqppBJYBldb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'openrouter/openai/gpt-4o-mini'\n",
        "dataset_no = 9 # index\n",
        "task_thread = 4\n",
        "sample = None\n",
        "print('model:', list_dataset[dataset_no])\n",
        "dataset = client.get_or_create_dataset(name=list_dataset[dataset_no])\n",
        "result = run_evaluation(MODEL, dataset, dataset_no, task_thread, sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f4a8b19f3d964dcdb685108c5190dab4",
            "7977c0d18923497191838e85322ae5c4",
            "026cd0dc33854162b855c337a07368a4",
            "0a0a219b86bb4287b192971a9ec71eaf",
            "c205458fb65c4703851498e10f867c1c",
            "f5271bd46876432c9bd9f3beb3f173e0",
            "41c4899af13d4fa4a395af0c4e35b1ad",
            "52fe4009251446c098fccc37186ef916",
            "3f0c788b62f44817bdfceaeb0a4c133d",
            "5f731349008e404fa40105e7407e6e30",
            "32cdb491f87b40e59d1175abedd347a2"
          ]
        },
        "id": "7DVFrLl8BD8C",
        "outputId": "2feff4e6-3af4-4476-c99c-84d8775f9759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: Model-5 ft | GoToCompany/llama3-8b-cpt-sahabatai-v1-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation:   0%|          | 0/103 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4a8b19f3d964dcdb685108c5190dab4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-30-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2172, in exception_type\n",
            "    raise APIError(\n",
            "litellm.exceptions.APIError: litellm.APIError: APIError: OpenrouterException - {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-30-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2172, in exception_type\n",
            "    raise APIError(\n",
            "litellm.exceptions.APIError: litellm.APIError: APIError: OpenrouterException - {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-30-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2172, in exception_type\n",
            "    raise APIError(\n",
            "litellm.exceptions.APIError: litellm.APIError: APIError: OpenrouterException - {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-30-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2172, in exception_type\n",
            "    raise APIError(\n",
            "litellm.exceptions.APIError: litellm.APIError: APIError: OpenrouterException - {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ Model-5 ft | GoToCompany/llama3-8b-cpt-sahabatai-v1-instruct (103 samples) ─╮\n",
              "│                                                                              │\n",
              "│ \u001b[1mTotal time:       \u001b[0m 00:05:24                                                  │\n",
              "│ \u001b[1mNumber of samples:\u001b[0m 103                                                       │\n",
              "│                                                                              │\n",
              "│ \u001b[1;32mhallucination_metric: 0.4950 (avg)\u001b[0m\u001b[31m - 2 failed\u001b[0m                                │\n",
              "│ \u001b[1;32manswer_relevance_metric: 0.8147 (avg)\u001b[0m\u001b[31m - 1 failed\u001b[0m                             │\n",
              "│ \u001b[1;32mmoderation_metric: 0.0000 (avg)\u001b[0m\u001b[31m - 1 failed\u001b[0m                                   │\n",
              "│ \u001b[1;32mUsefulnessMetric: 0.6408 (avg)\u001b[0m                                               │\n",
              "│                                                                              │\n",
              "╰──────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ Model-5 ft | GoToCompany/llama3-8b-cpt-sahabatai-v1-instruct (103 samples) ─╮\n",
              "│                                                                              │\n",
              "│ <span style=\"font-weight: bold\">Total time:       </span> 00:05:24                                                  │\n",
              "│ <span style=\"font-weight: bold\">Number of samples:</span> 103                                                       │\n",
              "│                                                                              │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">hallucination_metric: 0.4950 (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 2 failed</span>                                │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">answer_relevance_metric: 0.8147 (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 1 failed</span>                             │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">moderation_metric: 0.0000 (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 1 failed</span>                                   │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">UsefulnessMetric: 0.6408 (avg)</span>                                               │\n",
              "│                                                                              │\n",
              "╰──────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading results to Opik \u001b[33m...\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "View the results \u001b]8;id=407861;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=0197261d-8049-78af-a2a5-5e5c9cb3c4fa&dataset_id=019723cb-8c41-718a-9ef7-6dcc5c60607a&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=0197261d-8049-78af-a2a5-5e5c9cb3c4fa&dataset_id=019723cb-8c41-718a-9ef7-6dcc5c60607a&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 6"
      ],
      "metadata": {
        "id": "0c8KlFQ-xb-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model 6 base"
      ],
      "metadata": {
        "id": "P3xNG5YcD-Uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'openrouter/openai/gpt-4o-mini'\n",
        "dataset_no = 10 # index\n",
        "task_thread = 4\n",
        "sample = None\n",
        "print('model:', list_dataset[dataset_no])\n",
        "dataset = client.get_or_create_dataset(name=list_dataset[dataset_no])\n",
        "result = run_evaluation(MODEL, dataset, dataset_no, task_thread, sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cb66eb60b2534694af93da50df0c96d3",
            "448947bc6b734e618535f95bca15936c",
            "a502846b271a47ec903c186c9a488be7",
            "428874711bf14c5997e51b0cb3448473",
            "5736d85e260a4b2f8528cff3b0504015",
            "03eabaa93f5d4763b785ab4f6af42d0e",
            "6be7a79e460c4f85bfd8e37f0c616292",
            "0e7d4c366b56438c94d71377ac1ee98a",
            "64929cff9c4b49199bf5c50b1b4617fd",
            "ecd823e8dddd4205aa17b5967ddec1fe",
            "20316f0a4a37469a8e3d45c9a62f444e"
          ]
        },
        "id": "sZoo-Yz3D9pe",
        "outputId": "d59dbade-7dcb-4b8c-cd47-67c0ff1edeca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: Model-6 Base | GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation:   0%|          | 0/102 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb66eb60b2534694af93da50df0c96d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-30-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2172, in exception_type\n",
            "    raise APIError(\n",
            "litellm.exceptions.APIError: litellm.APIError: APIError: OpenrouterException - {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-30-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2172, in exception_type\n",
            "    raise APIError(\n",
            "litellm.exceptions.APIError: litellm.APIError: APIError: OpenrouterException - {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric UsefulnessMetric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/usefulness/metric.py\", line 80, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-30-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2172, in exception_type\n",
            "    raise APIError(\n",
            "litellm.exceptions.APIError: litellm.APIError: APIError: OpenrouterException - {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ Model-6 Base | GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct (102 samples) ─╮\n",
              "│                                                                                │\n",
              "│ \u001b[1mTotal time:       \u001b[0m 00:05:33                                                    │\n",
              "│ \u001b[1mNumber of samples:\u001b[0m 102                                                         │\n",
              "│                                                                                │\n",
              "│ \u001b[1;32mhallucination_metric: 0.3284 (avg)\u001b[0m                                             │\n",
              "│ \u001b[1;32manswer_relevance_metric: 0.8860 (avg)\u001b[0m\u001b[31m - 2 failed\u001b[0m                               │\n",
              "│ \u001b[1;32mmoderation_metric: 0.0000 (avg)\u001b[0m                                                │\n",
              "│ \u001b[1;32mUsefulnessMetric: 0.7931 (avg)\u001b[0m\u001b[31m - 1 failed\u001b[0m                                      │\n",
              "│                                                                                │\n",
              "╰────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ Model-6 Base | GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct (102 samples) ─╮\n",
              "│                                                                                │\n",
              "│ <span style=\"font-weight: bold\">Total time:       </span> 00:05:33                                                    │\n",
              "│ <span style=\"font-weight: bold\">Number of samples:</span> 102                                                         │\n",
              "│                                                                                │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">hallucination_metric: 0.3284 (avg)</span>                                             │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">answer_relevance_metric: 0.8860 (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 2 failed</span>                               │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">moderation_metric: 0.0000 (avg)</span>                                                │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">UsefulnessMetric: 0.7931 (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 1 failed</span>                                      │\n",
              "│                                                                                │\n",
              "╰────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading results to Opik \u001b[33m...\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "View the results \u001b]8;id=588740;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=0197262a-b93d-7897-a894-07e0be4103c2&dataset_id=019723cb-8d5b-7e96-8790-a198f338790d&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=0197262a-b93d-7897-a894-07e0be4103c2&dataset_id=019723cb-8d5b-7e96-8790-a198f338790d&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model ft 6"
      ],
      "metadata": {
        "id": "GeNWLaV7z89U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'openrouter/openai/gpt-4o-mini'\n",
        "dataset_no = 11 # index\n",
        "task_thread = 4\n",
        "sample = None\n",
        "print('model:', list_dataset[dataset_no])\n",
        "dataset = client.get_or_create_dataset(name=list_dataset[dataset_no])\n",
        "result = run_evaluation(MODEL, dataset, dataset_no, task_thread, sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317,
          "referenced_widgets": [
            "b2336def77d5464d9397f0d9f8e50c36",
            "264359af786e4617bfc1a9c5c60018bf",
            "5c29b73554f743e2b8a31d2dc47867c7",
            "d27c9bd1674a462e9bb7e967699e2371",
            "4d517158e1794885bbbc5fd839376edf",
            "6fa4ef428bf940aebfdc5a9d211a379b",
            "5a5a0aabd6b24b78acfd01fd3972f189",
            "012cd3f03cd9472cae61fa510b2cac20",
            "3eb3b13e7340478eaf5969aeab0d1714",
            "dda195906bae425d982d0cb0a236e421",
            "39a90ec2ef1d403b9633b908ab01b6df"
          ]
        },
        "id": "K7pq18Q8z-fT",
        "outputId": "1f7cf3bc-98e5-4706-9ef8-982e2327aca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: Model-6 ft | GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2336def77d5464d9397f0d9f8e50c36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Started logging traces to the \"Eval LLM NLP\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=019725d1-bc0d-7dab-9718-c74f285b0fff&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ Model-6 ft | GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct (100 samples) ─╮\n",
              "│                                                                              │\n",
              "│ \u001b[1mTotal time:       \u001b[0m 00:05:26                                                  │\n",
              "│ \u001b[1mNumber of samples:\u001b[0m 100                                                       │\n",
              "│                                                                              │\n",
              "│ \u001b[1;32mhallucination_metric: 0.5120 (avg)\u001b[0m                                           │\n",
              "│ \u001b[1;32manswer_relevance_metric: 0.8505 (avg)\u001b[0m                                        │\n",
              "│ \u001b[1;32mmoderation_metric: 0.0120 (avg)\u001b[0m                                              │\n",
              "│ \u001b[1;32mUsefulnessMetric: 0.7260 (avg)\u001b[0m                                               │\n",
              "│                                                                              │\n",
              "╰──────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ Model-6 ft | GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct (100 samples) ─╮\n",
              "│                                                                              │\n",
              "│ <span style=\"font-weight: bold\">Total time:       </span> 00:05:26                                                  │\n",
              "│ <span style=\"font-weight: bold\">Number of samples:</span> 100                                                       │\n",
              "│                                                                              │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">hallucination_metric: 0.5120 (avg)</span>                                           │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">answer_relevance_metric: 0.8505 (avg)</span>                                        │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">moderation_metric: 0.0120 (avg)</span>                                              │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">UsefulnessMetric: 0.7260 (avg)</span>                                               │\n",
              "│                                                                              │\n",
              "╰──────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading results to Opik \u001b[33m...\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "View the results \u001b]8;id=795130;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019725d1-b723-7739-812f-ad508a9453b5&dataset_id=019723cb-8e7a-72ec-901c-bb11d1f00abc&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019725d1-b723-7739-812f-ad508a9453b5&dataset_id=019723cb-8e7a-72ec-901c-bb11d1f00abc&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 7"
      ],
      "metadata": {
        "id": "xtFo5S4Z27Bw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model base 7"
      ],
      "metadata": {
        "id": "ghpsea_g79VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'openrouter/openai/gpt-4o-mini'\n",
        "dataset_no = 12 # index\n",
        "task_thread = 4\n",
        "sample = None\n",
        "print('model:', list_dataset[dataset_no])\n",
        "dataset = client.get_or_create_dataset(name=list_dataset[dataset_no])\n",
        "result = run_evaluation(MODEL, dataset, dataset_no, task_thread, sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ec64b03858c348f685a790ff9cee8f5a",
            "c7977e7ca7ab4902a9f47b5a9069c643",
            "6886b2dfecee4cc5a0d44404f0410870",
            "da85f85144b44d6eb84570d751ba4ee8",
            "2ea3542481ec4fbeb91fc12ba1ed46ea",
            "252323ecc4b540a2b375c47caf03f516",
            "f8ac0876102b4666a6207d3ddd89115a",
            "926cf1d062184ecba32f2a571ac78aaa",
            "0d0044d04c364de58e076c984e86fd3b",
            "3491f8eaf49d4a15bbdf1e38bfe2640f",
            "1aa2cb4b0aa243ca8ebebffa23923f62"
          ]
        },
        "id": "zQRAs2mg2-yn",
        "outputId": "930fdd66-f7aa-42e0-8891-fffa0f54e3e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: Model-7 Base | Yellow-AI-NLP/komodo-7b-base\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation:   0%|          | 0/104 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec64b03858c348f685a790ff9cee8f5a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-30-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2172, in exception_type\n",
            "    raise APIError(\n",
            "litellm.exceptions.APIError: litellm.APIError: APIError: OpenrouterException - {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-30-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2172, in exception_type\n",
            "    raise APIError(\n",
            "litellm.exceptions.APIError: litellm.APIError: APIError: OpenrouterException - {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ Model-7 Base | Yellow-AI-NLP/komodo-7b-base (104 samples) ─╮\n",
              "│                                                             │\n",
              "│ \u001b[1mTotal time:       \u001b[0m 00:06:39                                 │\n",
              "│ \u001b[1mNumber of samples:\u001b[0m 104                                      │\n",
              "│                                                             │\n",
              "│ \u001b[1;32mhallucination_metric: 0.6365 (avg)\u001b[0m                          │\n",
              "│ \u001b[1;32manswer_relevance_metric: 0.7218 (avg)\u001b[0m\u001b[31m - 1 failed\u001b[0m            │\n",
              "│ \u001b[1;32mmoderation_metric: 0.0136 (avg)\u001b[0m\u001b[31m - 1 failed\u001b[0m                  │\n",
              "│ \u001b[1;32mUsefulnessMetric: 0.4904 (avg)\u001b[0m                              │\n",
              "│                                                             │\n",
              "╰─────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ Model-7 Base | Yellow-AI-NLP/komodo-7b-base (104 samples) ─╮\n",
              "│                                                             │\n",
              "│ <span style=\"font-weight: bold\">Total time:       </span> 00:06:39                                 │\n",
              "│ <span style=\"font-weight: bold\">Number of samples:</span> 104                                      │\n",
              "│                                                             │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">hallucination_metric: 0.6365 (avg)</span>                          │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">answer_relevance_metric: 0.7218 (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 1 failed</span>            │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">moderation_metric: 0.0136 (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 1 failed</span>                  │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">UsefulnessMetric: 0.4904 (avg)</span>                              │\n",
              "│                                                             │\n",
              "╰─────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading results to Opik \u001b[33m...\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "View the results \u001b]8;id=787460;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019725ed-58bc-7000-bb90-bae78df71150&dataset_id=019723cb-8fa0-7fbc-9dfc-669606ac425f&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019725ed-58bc-7000-bb90-bae78df71150&dataset_id=019723cb-8fa0-7fbc-9dfc-669606ac425f&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model ft 7"
      ],
      "metadata": {
        "id": "wkzty12i8App"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'openrouter/openai/gpt-4o-mini'\n",
        "dataset_no = 13 # index\n",
        "task_thread = 4\n",
        "sample = None\n",
        "print('model:', list_dataset[dataset_no])\n",
        "dataset = client.get_or_create_dataset(name=list_dataset[dataset_no])\n",
        "result = run_evaluation(MODEL, dataset, dataset_no, task_thread, sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279,
          "referenced_widgets": [
            "85c6e8eca0d24ed0ad3beec204777886",
            "b5311a9ddf604384b02d058c71a6bf50",
            "2bbb427cca6e494c911684541813804d",
            "86f8d3c8f2fc4c6b89bd0d212327bd29",
            "a110e737742241c0babc249f6cd5f072",
            "b306550c369a4d34bb0bb58d2c819d2f",
            "5c5c3ade08854ac8b55558dd2e074a20",
            "e87f65fd79a64cd5988ea9840c5901e6",
            "f2077281d80a4223924f50f2091be55f",
            "5d5a81b20aae444380a2dc09d4ab2249",
            "c51c55b8b3aa46fcbe4db85691275a6f"
          ]
        },
        "id": "ujCJQxeb8BcS",
        "outputId": "ebd7a11f-b3a5-4269-950d-1b6a1575d0e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: Model-7 ft | Yellow-AI-NLP/komodo-7b-base\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85c6e8eca0d24ed0ad3beec204777886"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ Model-7 ft | Yellow-AI-NLP/komodo-7b-base (100 samples) ─╮\n",
              "│                                                           │\n",
              "│ \u001b[1mTotal time:       \u001b[0m 00:06:33                               │\n",
              "│ \u001b[1mNumber of samples:\u001b[0m 100                                    │\n",
              "│                                                           │\n",
              "│ \u001b[1;32mhallucination_metric: 0.5770 (avg)\u001b[0m                        │\n",
              "│ \u001b[1;32manswer_relevance_metric: 0.8215 (avg)\u001b[0m                     │\n",
              "│ \u001b[1;32mmoderation_metric: 0.0070 (avg)\u001b[0m                           │\n",
              "│ \u001b[1;32mUsefulnessMetric: 0.6800 (avg)\u001b[0m                            │\n",
              "│                                                           │\n",
              "╰───────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ Model-7 ft | Yellow-AI-NLP/komodo-7b-base (100 samples) ─╮\n",
              "│                                                           │\n",
              "│ <span style=\"font-weight: bold\">Total time:       </span> 00:06:33                               │\n",
              "│ <span style=\"font-weight: bold\">Number of samples:</span> 100                                    │\n",
              "│                                                           │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">hallucination_metric: 0.5770 (avg)</span>                        │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">answer_relevance_metric: 0.8215 (avg)</span>                     │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">moderation_metric: 0.0070 (avg)</span>                           │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">UsefulnessMetric: 0.6800 (avg)</span>                            │\n",
              "│                                                           │\n",
              "╰───────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading results to Opik \u001b[33m...\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "View the results \u001b]8;id=933261;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019725f3-796f-7188-8868-31088821ec52&dataset_id=019723cb-90fc-7ec4-9fad-17986ad09b88&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019725f3-796f-7188-8868-31088821ec52&dataset_id=019723cb-90fc-7ec4-9fad-17986ad09b88&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 8"
      ],
      "metadata": {
        "id": "0WdBobxA_KGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 8 base"
      ],
      "metadata": {
        "id": "GtkP-oYe_MeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'openrouter/openai/gpt-4o-mini'\n",
        "dataset_no = 14 # index\n",
        "task_thread = 4\n",
        "sample = None\n",
        "print('model:', list_dataset[dataset_no])\n",
        "dataset = client.get_or_create_dataset(name=list_dataset[dataset_no])\n",
        "result = run_evaluation(MODEL, dataset, dataset_no, task_thread, sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6e300af583fa4a8a8710003c90ee4c38",
            "eebd1245c55f44c28346ecd2563454b3",
            "d8ef5c819e9a49e49beaa4784ac09f66",
            "d0c0b2730c3d4f5980c295465d54dfc1",
            "fae8f47216644fe29b6f410a54c926cc",
            "192c0c19572f4467ae6ed90e2a538616",
            "4ed4dd97c5c34f88b8f79550949c9de1",
            "7b1d342532a44ad0bc544bda922c70e3",
            "cf8543ad2fdd457e98592efee8f9c618",
            "a72825493fdf41e19e9977cb5b9fcc08",
            "c1dad15e1f4d402799c47b0d2812afc2"
          ]
        },
        "id": "TbYRLZ4v_L_h",
        "outputId": "63251cf5-e894-4540-daa6-e3c604b931e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: Model-8 Base | Yellow-AI-NLP/komodo-7b-base\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation:   0%|          | 0/104 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e300af583fa4a8a8710003c90ee4c38"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-30-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2172, in exception_type\n",
            "    raise APIError(\n",
            "litellm.exceptions.APIError: litellm.APIError: APIError: OpenrouterException - {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ Model-8 Base | Yellow-AI-NLP/komodo-7b-base (104 samples) ─╮\n",
              "│                                                             │\n",
              "│ \u001b[1mTotal time:       \u001b[0m 00:06:50                                 │\n",
              "│ \u001b[1mNumber of samples:\u001b[0m 104                                      │\n",
              "│                                                             │\n",
              "│ \u001b[1;32mhallucination_metric: 0.6433 (avg)\u001b[0m                          │\n",
              "│ \u001b[1;32manswer_relevance_metric: 0.7250 (avg)\u001b[0m                       │\n",
              "│ \u001b[1;32mmoderation_metric: 0.0117 (avg)\u001b[0m\u001b[31m - 1 failed\u001b[0m                  │\n",
              "│ \u001b[1;32mUsefulnessMetric: 0.4788 (avg)\u001b[0m                              │\n",
              "│                                                             │\n",
              "╰─────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ Model-8 Base | Yellow-AI-NLP/komodo-7b-base (104 samples) ─╮\n",
              "│                                                             │\n",
              "│ <span style=\"font-weight: bold\">Total time:       </span> 00:06:50                                 │\n",
              "│ <span style=\"font-weight: bold\">Number of samples:</span> 104                                      │\n",
              "│                                                             │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">hallucination_metric: 0.6433 (avg)</span>                          │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">answer_relevance_metric: 0.7250 (avg)</span>                       │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">moderation_metric: 0.0117 (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 1 failed</span>                  │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">UsefulnessMetric: 0.4788 (avg)</span>                              │\n",
              "│                                                             │\n",
              "╰─────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading results to Opik \u001b[33m...\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "View the results \u001b]8;id=56913;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019725ff-7919-7927-8a92-616102595cac&dataset_id=019723cb-9236-75e2-9f5b-89363510da4d&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019725ff-7919-7927-8a92-616102595cac&dataset_id=019723cb-9236-75e2-9f5b-89363510da4d&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model ft 8"
      ],
      "metadata": {
        "id": "0cNxspLECLXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'openrouter/openai/gpt-4o-mini'\n",
        "dataset_no = 15 # index\n",
        "task_thread = 4\n",
        "sample = None\n",
        "print('model:', list_dataset[dataset_no])\n",
        "dataset = client.get_or_create_dataset(name=list_dataset[dataset_no])\n",
        "result = run_evaluation(MODEL, dataset, dataset_no, task_thread, sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e190a45d69ac43e6b8c5de5a5572d858",
            "77be98c82c6845a9a810567103ab0c10",
            "3ec72483447a4662863ce1fa37ab243d",
            "3baa14253d684e249a1c50dafd13c7a0",
            "5215a6d2f29240e6a97f945a0d0bc7a1",
            "f6b6152eda8f41349b7842fc012a7c23",
            "d5ebd97b353d43bd8808249a3358b2d0",
            "3ce059e4491e4f99b1bf8fa3af4f1ce1",
            "34b5cf7798cf48e2b49005fc63c9eb53",
            "26c05cd3d22b4717bc009fe04277b27e",
            "7c73098b73b04653bc92421f00db91c8"
          ]
        },
        "id": "lSj8LIxQCMhg",
        "outputId": "584d3b3e-7362-44b2-c9de-a5a7908e9f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: Model-8 ft | Yellow-AI-NLP/komodo-7b-base\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e190a45d69ac43e6b8c5de5a5572d858"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 126, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-30-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2172, in exception_type\n",
            "    raise APIError(\n",
            "litellm.exceptions.APIError: litellm.APIError: APIError: OpenrouterException - {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric moderation_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/moderation/metric.py\", line 78, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-30-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2172, in exception_type\n",
            "    raise APIError(\n",
            "litellm.exceptions.APIError: litellm.APIError: APIError: OpenrouterException - {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 159, in _make_common_sync_call\n",
            "    response = sync_httpx_client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 694, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 676, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2383, in completion\n",
            "    response = base_llm_http_handler.completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 447, in completion\n",
            "    response = self._make_common_sync_call(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 184, in _make_common_sync_call\n",
            "    raise self._handle_error(e=e, provider_config=provider_config)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 2048, in _handle_error\n",
            "    raise provider_config.get_error_class(\n",
            "litellm.llms.openrouter.common_utils.OpenRouterException: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/engine/engine.py\", line 58, in _evaluate_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 314, in wrapper\n",
            "    raise func_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/decorator/base_track_decorator.py\", line 287, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 91, in score\n",
            "    model_output = self._model.generate_string(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-30-92475b59c1a9>\", line 17, in generate_string\n",
            "    return self.client.query(input, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 40, in query\n",
            "    return self._get_litellm_response(query, system)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-29-ec59a9114300>\", line 21, in _get_litellm_response\n",
            "    response = litellm.completion(model=self.model, messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1283, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1161, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3241, in completion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2172, in exception_type\n",
            "    raise APIError(\n",
            "litellm.exceptions.APIError: litellm.APIError: APIError: OpenrouterException - {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ Model-8 ft | Yellow-AI-NLP/komodo-7b-base (100 samples) ─╮\n",
              "│                                                           │\n",
              "│ \u001b[1mTotal time:       \u001b[0m 00:05:31                               │\n",
              "│ \u001b[1mNumber of samples:\u001b[0m 100                                    │\n",
              "│                                                           │\n",
              "│ \u001b[1;32mhallucination_metric: 0.4667 (avg)\u001b[0m\u001b[31m - 1 failed\u001b[0m             │\n",
              "│ \u001b[1;32manswer_relevance_metric: 0.8318 (avg)\u001b[0m\u001b[31m - 1 failed\u001b[0m          │\n",
              "│ \u001b[1;32mmoderation_metric: 0.0020 (avg)\u001b[0m\u001b[31m - 1 failed\u001b[0m                │\n",
              "│ \u001b[1;32mUsefulnessMetric: 0.7080 (avg)\u001b[0m                            │\n",
              "│                                                           │\n",
              "╰───────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ Model-8 ft | Yellow-AI-NLP/komodo-7b-base (100 samples) ─╮\n",
              "│                                                           │\n",
              "│ <span style=\"font-weight: bold\">Total time:       </span> 00:05:31                               │\n",
              "│ <span style=\"font-weight: bold\">Number of samples:</span> 100                                    │\n",
              "│                                                           │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">hallucination_metric: 0.4667 (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 1 failed</span>             │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">answer_relevance_metric: 0.8318 (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 1 failed</span>          │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">moderation_metric: 0.0020 (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 1 failed</span>                │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">UsefulnessMetric: 0.7080 (avg)</span>                            │\n",
              "│                                                           │\n",
              "╰───────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading results to Opik \u001b[33m...\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "View the results \u001b]8;id=575827;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=0197260f-9417-72c9-9c36-dc9a732f4105&dataset_id=019723cb-9366-7982-93cb-b4cdf0119083&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=0197260f-9417-72c9-9c36-dc9a732f4105&dataset_id=019723cb-9366-7982-93cb-b4cdf0119083&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RJyJeo3uCOZH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}